<!DOCTYPE html>
<!--[if IE 9]><html class="ie9" lang="en"><![endif]-->
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <meta name="citation_pii" content="S1537511018303180">
<meta name="citation_issn" content="1537-5110">
<meta name="citation_volume" content="174">
<meta name="citation_lastpage" content="133">
<meta name="citation_publisher" content="Academic Press">
<meta name="citation_firstpage" content="126">
<meta name="citation_journal_title" content="Biosystems Engineering">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.biosystemseng.2018.07.003">
<meta name="dc.identifier" content="10.1016/j.biosystemseng.2018.07.003">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="A stereovision system for the in-field estimation of trees parameters such as height and diameter is proposed. The system includes a specifically deve…">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018X00075-cov150h.gif">
<meta name="citation_title" content="Digital stereovision system for dendrometry, georeferencing and data management">
<meta property="og:title" content="Digital stereovision system for dendrometry, georeferencing and data management">
<meta name="citation_publication_date" content="2018/10/01">
<meta name="citation_online_date" content="2018/07/18">
<meta name="citation_pdf_url" content="https://www.sciencedirect.com/science/article/pii/S1537511018303180/pdfft?md5=8899bcbf3b23c97f1028cdb556afda8c&amp;pid=1-s2.0-S1537511018303180-main.pdf">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR">
      <title>Digital stereovision system for dendrometry, georeferencing and data management - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S1537511018303180">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="arp.css">
      <link rel="dns-prefetch" href="https://w.usabilla.com/">
      <link rel="dns-prefetch" href="https://www.deepdyve.com/">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com/">
      <iframe src="javascript:false" title="" style="width: 0px; height: 0px; border: 0px none; display: none;"></iframe><script>
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Biosystems Engineering","auth":"AE"};
        window.pageData = {"content":[{"entitlementType":"package","format":"MIME-XHTML","id":"sd:article:pii:S1537511018303180","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"1537-5110","volumeNumber":"174","suppl":"C"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1551002788676,"loadTime":""},"visitor":{"accessType":"ae:ANON_IP","accountId":"ae:31739","accountName":"ae:ASCR - CTR South Bohemian Biology","loginStatus":"anonymous","userId":"ae:622212","ipAddress":"147.231.250.202","appSessionId":"a30863ea9ce018477b4ac8d08be0e378262fgxrqb"}};
        window.arp = {
          config: {"reduxLogging":false,"assetsBaseUrl":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/cf04e09c260298895aba5a2dbd00cefe1f94c5b5","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","bosUrl":"https://feedback.recs.d.elsevier.com/raw/events","bosTimeOut":60000,"strictMode":false,"facebookAppId":"269707397153187","enableGoogleScholarLinks":true,"enableEzProxyForEnhancedReader":false},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
      </script>
      
      <!-- begin mPulse embed code -->
      <script>
        (function(){
          if(window.BOOMR && window.BOOMR.version){return;}
          var dom,doc,where,iframe = document.createElement('iframe'),win = window;

          function boomerangSaveLoadTime(e) {
            win.BOOMR_onload=(e && e.timeStamp) || new Date().getTime();
          }
          if (win.addEventListener) {
            win.addEventListener("load", boomerangSaveLoadTime, false);
          } else if (win.attachEvent) {
            win.attachEvent("onload", boomerangSaveLoadTime);
          }

          iframe.src = "javascript:false";
          iframe.title = ""; iframe.role="presentation";
          (iframe.frameElement || iframe).style.cssText = "width:0;height:0;border:0;display:none;";
          where = document.getElementsByTagName('script')[0];
          where.parentNode.insertBefore(iframe, where);

          try {
            doc = iframe.contentWindow.document;
          } catch(e) {
            dom = document.domain;
            iframe.src="javascript:var d=document.open();d.domain='"+dom+"';void(0);";
            doc = iframe.contentWindow.document;
          }
          doc.open()._l = function() {
            var js = this.createElement("script");
            if(dom) this.domain = dom;
            js.id = "boomr-if-as";
            js.src = 'https://c.go-mpulse.net/boomerang/2FBN2-NKMGU-EJKY8-ZANKZ-SUJZF';
            BOOMR_lstart=new Date().getTime();
            this.body.appendChild(js);
          };
          doc.write('<body onload="document._l();">');
          doc.close();
        })();
      </script>
      <!-- end mPulse embed code -->
    <script src="satellite-5b3b560664746d57b70018c6.js"></script><script src="s-code-contents-9c0358adbc3b5986e210099b3bf1d427fc5bd286.js"></script><link rel="preload" href="integrator.js"><script type="text/javascript" src="integrator.js"></script><link rel="preload" href="integrator_002.js"><script type="text/javascript" src="integrator_002.js"></script><script src="pubads_impl_308.js" async=""></script><link id="plx-css-summary" type="text/css" rel="stylesheet" href="summary.css"><script type="text/javascript" src="jquery.js"></script><script type="text/javascript" src="xss.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css" media="print">.usabilla_live_button_container { display: none; }</style></head>
    <body><div style="display: none;" id="lightningjs-usabilla_live"><div><iframe id="lightningjs-frame-usabilla_live" frameborder="0"></iframe></div></div>
      <a class="sr-only sr-only-focusable" href="#app">Skip to main content</a>
      <!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service.elsevier.com/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
      <div data-iso-key="_0"><div class="App" id="app" data-reactroot=""><div class="page"><section><div id="header"><div class="u-bg-white" style="overflow:visible" role="banner"><div class="els-header" style="min-height:80px"><a id="els-main-title-link" href="https://www.sciencedirect.com/"><svg id="els-header-wordmark" class="u-fill-orange u-margin-l-top u-margin-s-left u-margin-l-left-from-sm" viewBox="-3334 3439.4 163 26" style="width:163px;height:24px"><title>ScienceDirect</title><g transform="scale(.8125,.8125)"><path d="M-4099.8,4240.4c0-1.8,1-3.6,4.4-3.6c1.7,0,3.7,0.5,5.5,1.6l0.2-3.1c-1.7-0.7-3.5-1.1-5.6-1.1 c-5.5,0-7.8,2.9-7.8,6.4c0,6.6,10.4,7.2,10.4,11.7c0,1.8-1.4,3.6-4.6,3.6c-2,0-4-0.7-5.6-1.6l-0.4,3.1c1.8,0.9,4.2,1.2,6.1,1.2 c5,0,7.9-2.9,7.9-6.5C-4089.4,4245.8-4099.8,4244.9-4099.8,4240.4"></path><path id="c" d="M-4080.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4083.1,4244.3-4080.6,4243.1-4080.3,4242.9"></path><path id="i" d="M-4068,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-4066,4234.1-4067,4233.1-4068,4233.1 M-4069.5,4258.1h3v-17.7h-3V4258.1z"></path><path id="e" d="M-4057.1,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4058.7,4244.4-4057.4,4243.3-4057.1,4243.1z M-4047.5,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4 c0,5.8,3.5,9.2,7.9,9.2c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-4047.5,4248.9"></path><path d="M-4034.8,4240c-2.6,0-4.3,1.3-5.7,3.1l-0.5-2.6h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.5v12.8h3v-11.9 c0.8-1.1,2.5-2.9,2.9-3.1c0.3-0.2,1.5-0.5,2.5-0.5c2.7,0,2.9,1.4,3,4c0,1.4,0,3.7,0,3.7c0,3.5-0.1,7.6-0.1,7.6h3 c0,0,0.1-5.3,0.1-8.2c0-1.8,0-3.5-0.1-5.3C-4029.5,4241-4031.6,4240-4034.8,4240"></path><path d="M-3982.5,4255.6h-4.4v-18.6h4.8c6.4,0,8.2,5.2,8.2,9.2C-3973.8,4252.2-3976.5,4255.6-3982.5,4255.6z M-3981.6,4234.6h-8.4v23.5h8.1c8.6,0,11.5-6.7,11.5-11.9C-3970.4,4240.9-3973.2,4234.6-3981.6,4234.6"></path><path d="M-3950.5,4240c-1.9,0-3.4,1.7-4,3.2l-0.5-2.8h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.4v12.8h3v-11.2 c0.6-1.5,2-4.4,3.7-4.4c1.1,0,1.2,1.2,1.2,1.4l2.5-0.7v-0.2c0,0,0-0.2-0.1-0.5C-3947.4,4240.9-3948.5,4240-3950.5,4240"></path><path d="M-3903.5,4255.2c-1.1,0.4-2,0.8-3,0.8c-1.4,0-1.9-0.8-1.9-2.8v-10.4h4.6v-2.3h-4.6v-4.7h-2.9v4.7h-3.2v2.3h3.2 v11.4c0,3.1,1.6,4.4,3.9,4.4c1.4,0,3-0.5,4.1-0.9L-3903.5,4255.2"></path><g><path d="M-3923.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-3926.1,4244.3-3923.6,4243.1-3923.3,4242.9"></path></g><g><path d="M-3941.6,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-3943.2,4244.4-3941.8,4243.3-3941.6,4243.1z M-3932,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3932,4248.9"></path></g><g><path d="M-3964.5,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-3962.5,4234.1-3963.4,4233.1-3964.5,4233.1 M-3965.9,4258.1h3v-17.7h-3V4258.1z"></path></g><g><path d="M-4004.4,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4006,4244.4-4004.6,4243.3-4004.4,4243.1z M-3994.7,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3994.7,4248.9"></path></g><g><path d="M-4019.1,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4021.8,4244.3-4019.4,4243.1-4019.1,4242.9"></path></g></g></svg></a><div class="move-right u-display-inline-block"><nav class="u-clr-grey8 u-show-from-md"><div class="u-display-inline-block" style="margin-top:29px"><span><span class="u-margin-l-right"><a class="anchor qa-journals-and-books u-margin-l-right anchor-has-inherit-color" href="https://www.sciencedirect.com/browse/journals-and-books" id="els-header-navigation-section-journals-and-books" style="border-bottom:"><span class="anchor-text">Journals &amp; Books</span></a></span><a class="anchor qa-journals-and-books u-margin-l-right anchor-has-inherit-color" href="https://www.sciencedirect.com/user/register?returnURL=%2Fscience%2Farticle%2Fpii%2FS1537511018303180" id="els-header-navigation-section-register"><span class="anchor-text">Register</span></a></span><a class="anchor qa-no-js-fallback-link anchor-has-inherit-color" href="https://www.sciencedirect.com/user/login?returnURL=%2Fscience%2Farticle%2Fpii%2FS1537511018303180" id="els-header-user-sign-in"><span class="anchor-text">Sign in</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a><span><a class="anchor u-margin-l-hor help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" id="help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text"><svg focusable="false" viewBox="0 0 114 128" style="margin-top:-10px" width="21.375" height="24" class="icon icon-help"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg></span></a></span></div></nav></div><div class="u-hide-from-md move-right"><ul><li style="list-style:none;display:inline-block;margin-top:32px" class="u-margin-s-hor u-margin-l-hor-from-sm"><div class="hamburger-button"><button class="button-link button-link-primary" aria-label="Mobile menu" aria-expanded="false" type="button"><svg viewBox="0 0 40 18" width="40" height="18" y="52"><path d="M0 16h40v2H0zm0-8h40v2H0zm0-8h40v2H0z"></path></svg><span class="button-link-text"></span></button></div></li></ul><div style="bottom:0;left:0;position:fixed;top:0;width:100%;z-index:70;opacity:.8" class="u-bg-grey1 u-display-none"></div><div id="mobile-menu" style="overflow:auto;position:fixed;width:288px;right:0;top:0;z-index:1000;height:100%" aria-label="Mobile menu" class="u-bg-grey7 u-clr-grey1 u-display-none" role="navigation"><div class="u-bg-black panel-s"></div><div class="u-bg-grey8 panel-s"><ul><li style="list-style:none"><a class="anchor journals-and-books-link u-padding-xs-top anchor-has-inherit-color" href="https://www.sciencedirect.com/browse/journals-and-books" style="border-bottom:" id="mobile-journals-and-books-link"><span class="anchor-text">Journals &amp; Books</span></a></li><li style="list-style:none"><a class="anchor u-padding-xs-top anchor-has-inherit-color" href="https://www.sciencedirect.com/user/register?returnURL=%2Fscience%2Farticle%2Fpii%2FS1537511018303180" id="mobile-register-link"><span class="anchor-text">Register</span></a></li></ul></div><div class="panel-s u-bg-grey7"><ul class="text-xs"></ul><ul class="u-margin-s-top text-s"><li style="list-style:none"><a class="anchor anchor-has-inherit-color" href="https://www.sciencedirect.com/user/login?returnURL=%2Fscience%2Farticle%2Fpii%2FS1537511018303180" id="mobile-sign-in-out-link" rel="nofollow"><span class="anchor-text">Sign In</span></a></li><span><a class="anchor u-padding-xs-top text-s help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" id="mobile-help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text">Help</span></a></span></ul></div></div></div></div></div></div><div class="Article" id="mathjax-container"><div class="sticky-outer-wrapper"><div class="sticky-inner-wrapper" style="position: relative; z-index: 1; transform: translate3d(0px, 0px, 0px);"><div class="Toolbar" role="region" aria-label="download options and search"><div class="toolbar-container"><div class="u-show-from-lg col-lg-6">&nbsp;</div><div class="buttons text-s pull-left pad-left"><button class="button show-toc-button button-anchor u-hide-from-lg u-margin-s-right" type="button"><svg focusable="false" viewBox="0 0 104 128" width="19.5" height="24" class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg><span class="button-text"><span>Outline</span></span></button><div class="PdfDownloadButton"><a class="anchor u-margin-s-right" href="https://www.sciencedirect.com/science/article/pii/S1537511018303180/pdfft?md5=8899bcbf3b23c97f1028cdb556afda8c&amp;pid=1-s2.0-S1537511018303180-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text"><span class="pdf-download-label u-show-inline-from-lg">Download PDF</span><span class="pdf-download-label-short u-hide-from-lg">Download</span></span></a></div><div class="Social" id="social"><div class="popover social-popover" id="social-popover" aria-label="Share article on social media"><div id="popover-trigger-social-popover"><button class="button button-anchor" role="button" aria-haspopup="true" aria-expanded="false" type="button"><span class="button-text">Share</span></button></div></div></div><div class="ExportCitation" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover" aria-label="Export or save citation"><div id="popover-trigger-export-citation-popover"><button class="button button-anchor" role="button" aria-haspopup="true" aria-expanded="false" type="button"><span class="button-text">Export</span></button></div></div></div></div><div class="quick-search-container pull-right pad-right u-show-from-md"><form id="quick-search" class="QuickSearch u-margin-xs-right" action="/search/advanced" method="get"><input class="query" aria-label="Search ScienceDirect" name="qs" placeholder="Search ScienceDirect" type="text"><button class="button button-primary" type="submit" aria-label="Submit search"><span class="button-text"><svg focusable="false" viewBox="0 0 100 128" height="20" width="18.75" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button><a class="advanced-search-link" href="https://www.sciencedirect.com/search/advanced">Advanced</a><input name="origin" value="article" type="hidden"><input name="zone" value="qSearch" type="hidden"></form></div></div></div></div></div><div class="article-wrapper u-padding-m-top grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li><a href="#abs0015" title="Highlights">Highlights</a></li><li><a href="#kwrds0010" title="Keywords">Keywords</a></li><li><a href="#nomen0010" title="Nomenclature">Nomenclature</a></li><li><a href="#sec1" title="1. Introduction">1. Introduction</a></li><li><a href="#sec2" title="2. Materials and methods">2. Materials and methods</a></li><li><a href="#sec3" title="3. Results">3. Results</a></li><li><a href="#sec4" title="4. Discussion">4. Discussion</a></li><li><a href="#sec5" title="5. Conclusions">5. Conclusions</a></li><li><a href="#ack0010" title="Acknowledgements">Acknowledgements</a></li><li><a href="#cebib0010" title="References">References</a></li></ol><button class="button button-anchor" type="button"><span class="button-text">Show full outline</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (5)</h2><ol><li><a href="#fig1"><div><img alt="Fig.1. Stereovision system consisting of two high resolution SLR cameras (Canon EOS…" src="1-s2.gif" style="max-width: 173px; max-height: 163px;"></div></a></li><li><a href="#fig2"><div><img alt="Fig.2. Device for simultaneous shooting control over the two high resolution SLR…" src="1-s2_005.gif" style="max-width: 132px; max-height: 163px;"></div></a></li><li><a href="#fig3"><div><img alt="Fig.3. Scatter plots for the estimation of the average percentage error between the…" src="1-s2_002.gif" style="max-width: 219px; max-height: 111px;"></div></a></li><li><a href="#fig4"><div><img alt="Fig.4. Scatters plots of the testing phase" src="1-s2_004.gif" style="max-width: 168px; max-height: 164px;"></div></a></li><li><a href="#fig5"><div><img alt="Fig.5. Screen images from the SmartTree application developed in this study for…" src="1-s2_006.gif" style="max-width: 219px; max-height: 108px;"></div></a></li></ol><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role="main" lang="en"><noscript>JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.</noscript><div class="Publication" id="publication"><div class="publication-brand u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/15375110"><img class="publication-brand-image" src="elsevier-non-solus.png" alt="Elsevier"></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="publication-title-link" title="Go to Biosystems Engineering on ScienceDirect" href="https://www.sciencedirect.com/science/journal/15375110">Biosystems Engineering</a></h2><div class="text-xs"><a title="Go to table of contents for this volume/issue" href="https://www.sciencedirect.com/science/journal/15375110/174/supp/C">Volume 174</a>, <!-- -->October 2018<!-- -->, Pages 126-133</div></div><div class="publication-cover u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/15375110/174/supp/C"><img class="publication-cover-image" src="1-s2_003.gif" alt="Biosystems Engineering"></a></div></div><h1 class="Head u-font-serif u-h2 u-margin-s-ver"><div class="article-dochead"><span>Research Paper</span></div><span class="title-text">Digital stereovision system for dendrometry, georeferencing and data management</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups text-xs"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><a class="author size-m workspace-trigger" name="bau1" href="#%21"><span class="content"><span class="text given-name">Corrado</span><span class="text surname">Costa</span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 106 128" width="19.875" height="24" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable="false" viewBox="0 0 102 128" width="19.125" height="24" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name="bau2" href="#%21"><span class="content"><span class="text given-name">Simone</span><span class="text surname">Figorilli</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a><a class="author size-m workspace-trigger" name="bau3" href="#%21"><span class="content"><span class="text given-name">Andrea Rosario</span><span class="text surname">Proto</span><span class="author-ref" id="baff2"><sup>b</sup></span></span></a><a class="author size-m workspace-trigger" name="bau4" href="#%21"><span class="content"><span class="text given-name">Giacomo</span><span class="text surname">Colle</span><span class="author-ref" id="baff3"><sup>c</sup></span></span></a><a class="author size-m workspace-trigger" name="bau5" href="#%21"><span class="content"><span class="text given-name">Giulio</span><span class="text surname">Sperandio</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a><a class="author size-m workspace-trigger" name="bau6" href="#%21"><span class="content"><span class="text given-name">Pietro</span><span class="text surname">Gallo</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a><a class="author size-m workspace-trigger" name="bau7" href="#%21"><span class="content"><span class="text given-name">Francesca</span><span class="text surname">Antonucci</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a><a class="author size-m workspace-trigger" name="bau8" href="#%21"><span class="content"><span class="text given-name">Federico</span><span class="text surname">Pallottino</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a><a class="author size-m workspace-trigger" name="bau9" href="#%21"><span class="content"><span class="text given-name">Paolo</span><span class="text surname">Menesatti</span><span class="author-ref" id="baff1"><sup>a</sup></span></span></a></div></div></div><button class="show-hide-details u-font-sans" type="button"><svg viewBox="0 0 9 9" class="icon-expand"><path d="M5 7H4V5H2V4h2V2h1v2h2v1H5z"></path><path d="M0 0v9h9V0zm1 1h7v7H1z"></path></svg>Show more</button></div><div class="DoiLink" id="doi-link"><a class="doi" href="https://doi.org/10.1016/j.biosystemseng.2018.07.003" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi.org/10.1016/j.biosystemseng.2018.07.003</a><a class="rights-and-content" target="_blank" rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1537511018303180&amp;orderBeanReset=true">Get rights and content</a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author-highlights" id="abs0015" lang="en"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Highlights</h2><div id="abssec0015"><p id="abspara0015"><dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0010">Stereovision for in-field estimation of tree height and diameter.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0015">Mobile Android app for the management and georeferencing of stereo images.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0020">Comparison of measurements - reference (felled trees) and stereovision.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0025">No significant differences between stereovision and traditional methods.</p></dd></dl></p></div></div><div class="abstract author" id="abs0010" lang="en"><div id="abssec0010"><p id="abspara0010">A <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a><span><span><span>
 system for the in-field estimation of trees parameters such as height 
and diameter is proposed. The system includes a specifically developed 
mobile application for the management and <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a>
 of stereo images. Stereo imaging allows the measurement of the distance
 between two points using triangulation formulas for the extraction of </span><a href="https://www.sciencedirect.com/topics/engineering/three-dimensional-computer-graphics" title="Learn more about Three Dimensional Computer Graphics">three-dimensional</a> coordinates. The methodology is structured following three phases: training using </span><a href="https://www.sciencedirect.com/topics/engineering/calibration-system" title="Learn more about Calibration System">system calibration</a><span>
 through stereovision analysis of known artificial known objects; 
testing using measurements of standing tree diameters and heights 
acquired through stereovision <a href="https://www.sciencedirect.com/topics/engineering/laser-system" title="Learn more about Laser System">system, laser</a><span> rangefinder (height) and tree <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/calipers" title="Learn more about Calipers">calliper</a>
 (diameter); field application testing using direct height and diameter 
measurements in natural and urban woods. For this last phase an Android 
application was developed. The results show that the error between 
direct measurements and those measured with both stereovision and 
traditional reference methods (laser rangefinder and tree calliper) were
 quite low: 6.8&nbsp;±&nbsp;6.6% between direct and laser rangefinder 
height measurements; 5.8&nbsp;±&nbsp;5.5% between direct and 
stereovision height measurements; 4.2&nbsp;±&nbsp;3.0% between direct 
and stereovision diameter measurements. No significant difference was 
found between the different methods for estimating height and diameter. 
Around 200 images matched to stereovision acquisitions were acquired and
 georeferenced using the application.</span></span></span></p></div></div></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href="https://www.sciencedirect.com/science/article/pii/S1537511018300618"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text"><strong>Previous </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary" href="https://www.sciencedirect.com/science/article/pii/S1537511017305937"><span class="button-alternative-text"><strong>Next </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="kwrds0010" class="keywords-section"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="kwrd0010" class="keyword"><span>Stereo images</span></div><div id="kwrd0015" class="keyword"><span>Image analysis</span></div><div id="kwrd0020" class="keyword"><span>Tree diameters</span></div><div id="kwrd0025" class="keyword"><span>Tree heights</span></div><div id="kwrd0030" class="keyword"><span>Android application</span></div></div></div><div class="Body u-font-serif" id="body"><section id="nomen0010"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Nomenclature</h2><dl id="dlist0010"><dt>ALS</dt><dd class="u-margin-xxl-left"><p id="p0030">Airborne Laser Scanning</p></dd><dt>CSV</dt><dd class="u-margin-xxl-left"><p id="p0035">Comma-Separated Values</p></dd><dt>DGNSS</dt><dd class="u-margin-xxl-left"><p id="p0040">Differential Global Navigation Satellite Systems</p></dd><dt>GNSS</dt><dd class="u-margin-xxl-left"><p id="p0045">Global Navigation Satellite Systems</p></dd><dt>GPS</dt><dd class="u-margin-xxl-left"><p id="p0050"><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/global-positioning-systems" title="Learn more about Global Positioning Systems">Global Positioning System</a></p></dd><dt>JPG</dt><dd class="u-margin-xxl-left"><p id="p0055">Joint Photographic Group</p></dd><dt>JSON</dt><dd class="u-margin-xxl-left"><p id="p0060">JavaScript Object Notation</p></dd><dt>KML</dt><dd class="u-margin-xxl-left"><p id="p0065">Keyhole Mark-up Language</p></dd><dt>LAI</dt><dd class="u-margin-xxl-left"><p id="p0070"><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/leaf-area-index" title="Learn more about Leaf Area Index">Leaf Area Index</a></p></dd><dt>LIDAR</dt><dd class="u-margin-xxl-left"><p id="p0075"><a href="https://www.sciencedirect.com/topics/engineering/light-detection-and-ranging" title="Learn more about light detection and ranging">Light Detection And Ranging</a></p></dd><dt>PHP</dt><dd class="u-margin-xxl-left"><p id="p0080">Hypertext <a href="https://www.sciencedirect.com/topics/engineering/preprocessor" title="Learn more about Preprocessor">Pre-processor</a></p></dd><dt>RGB</dt><dd class="u-margin-xxl-left"><p id="p0085">Red Green Blue</p></dd><dt>SD</dt><dd class="u-margin-xxl-left"><p id="p0090">Standard Deviation</p></dd><dt>SLR</dt><dd class="u-margin-xxl-left"><p id="p0095">Single-Lens <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/reflexes" title="Learn more about Reflexes">Reflex</a></p></dd><dt>TLS</dt><dd class="u-margin-xxl-left"><p id="p0100">Terrestrial <a href="https://www.sciencedirect.com/topics/engineering/laser-scanner" title="Learn more about Laser Scanner">Laser Scanner</a></p></dd><dt>VTA</dt><dd class="u-margin-xxl-left"><p id="p0105">Visual Tree Assessment</p></dd></dl></section><div><section id="sec1"><h2 id="sectitle0025" class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><p id="p0110"><span><span>Wood volume is one of the most important parameters required for the evaluation of forests. It is used to assess <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/tree-growth" title="Learn more about Tree Growth">tree growth</a> and to monitor </span><a href="https://www.sciencedirect.com/topics/engineering/natural-resources" title="Learn more about Natural Resources">natural resources</a> in order to apply proper maintenance (</span><a name="bbib17" href="#bib17" class="workspace-trigger">Herrera, Pajares, Guijarro, Ruz, &amp; Cruz, 2011</a><span>).
 To calculate wood volume accurately is necessary to precisely acquire 
dendrometry parameters such as height and diameter. These parameters are
 also used to indicate the status of <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/tree-health" title="Learn more about Tree Health">tree health</a>, when referred to a population, or to classify the mechanical danger in relation to the probability of tree toppling (</span><a name="bbib26" href="#bib26" class="workspace-trigger">Smiley, Fraedrich, &amp; Fengler, 2007</a>).
 Also, identification of timber species and estimation of the quantity 
and quality are of timber are critical for quantifying the productive 
value of a forest (<a name="bbib22" href="#bib22" class="workspace-trigger">Proto, Macrì, Bernardini, Russo, &amp; Zimbalatti, 2017</a>). Furthermore, the same parameters are crucial in timber industry for various commercial and logistic purposes (<a name="bbib15" href="#bib15" class="workspace-trigger">Gutzeit &amp; Voskamp, 2012</a>).</p><p id="p0115"><span><a href="https://www.sciencedirect.com/topics/engineering/computer-vision" title="Learn more about Computer Vision">Computer vision</a><span>
 applications are rapidly increasing in their scope and performance and 
have become important ready-to use tools in the forestry sector. Some 
applications are available to calculate <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/leaf-area-index" title="Learn more about Leaf Area Index">leaf area index</a> (LAI; </span></span><a name="bbib5" href="#bib5" class="workspace-trigger">Chianucci &amp; Cutini, 2013</a>), texture (<a name="bbib1" href="#bib1" class="workspace-trigger">Bai, Wang, &amp; Wang, 2005</a><span><span>), <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/basal-area" title="Learn more about Basal Area">basal area</a> and </span><a href="https://www.sciencedirect.com/topics/engineering/diameter-distribution" title="Learn more about Diameter Distribution">diameter distribution</a> (</span><a name="bbib14" href="#bib14" class="workspace-trigger">Gobakken &amp; Næsset, 2004</a>.
 These techniques, also provide solutions for automatically determining 
log quality and grade, without the necessity to climb tree, thereby 
improving the accuracy and precision of upper diameters when estimating 
standing trees (<a name="bbib17" href="#bib17" class="workspace-trigger">Herrera et&nbsp;al., 2011</a>). For example, in the study by <a name="bbib3" href="#bib3" class="workspace-trigger">Brownlie, Carson, Firth, and Goulding (2007)</a><span>,
 a digital image-based dendrometry system was developed to provide 
accurate dimensional and positional measurements of the branches, whorls
 and <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/stems" title="Learn more about Stems">stems</a>
 of standing trees. This included sweep and three-dimensional (3D) 
position. Tree stereo digital images are acquired and some field 
parameters are measured for their association with a particular 
image/object environment. </span><a name="bbib12" href="#bib12" class="workspace-trigger">Firth, Brownlie, and Carson (2000)</a> acquired high resolution RGB pictures of tree stems to describe the camera-to-object (<em>i.e.</em>, tree) geometry.</p><p id="p0120"><span><span>Although computer vision techniques are widely available and applied for the volume calculation of the <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/wood-logs" title="Learn more about Wood Logs">wood log</a> </span><a href="https://www.sciencedirect.com/topics/engineering/piles" title="Learn more about Piles">piles</a>, only a few studies report the use of these technologies to calculate dendrometry parameters for single standing trees (</span><a name="bbib3" href="#bib3" class="workspace-trigger">Brownlie et&nbsp;al., 2007</a>, <a name="bbib12" href="#bib12" class="workspace-trigger">Firth et&nbsp;al., 2000</a>). <a name="bbib11" href="#bib11" class="workspace-trigger">Fink (2004)</a><span> applied classical vision algorithms and <a href="https://www.sciencedirect.com/topics/engineering/active-contour" title="Learn more about Active Contour">active contours</a><span><span> to identify log <a href="https://www.sciencedirect.com/topics/engineering/cut-surface" title="Learn more about Cut Surface">cut surfaces</a>. The procedure they proposed worked semi-automatically making assumptions on wood type, </span><a href="https://www.sciencedirect.com/topics/engineering/lighting-condition" title="Learn more about Lighting Condition">lighting conditions</a> and image quality. The method proposed by </span></span><a name="bbib10" href="#bib10" class="workspace-trigger">Dahl, Guo, and Madsen (2006)</a><span>, consisted of a measuring system based on <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a>
 with a structure relying on two cameras. The area of wood section was 
calculated beforehand by using stereovision algorithms. This method 
estimated the percentage of wood in the measurement of wood log piles 
and worked only with clear images without disturbing factors. </span><a name="bbib17" href="#bib17" class="workspace-trigger">Herrera et&nbsp;al. (2011)</a><span><span> present a strategy for computing disparity maps from hemispherical stereo <a href="https://www.sciencedirect.com/topics/engineering/obtained-image" title="Learn more about Obtained Image">images obtained</a> with dedicated fish-eye lenses. To obtain appropriate measures of vegetation structure at multiple </span><a href="https://www.sciencedirect.com/topics/engineering/spatial-scale" title="Learn more about Spatial Scale">spatial scales</a> in forest and in urban environments is normally challenging.</span></p><p id="p0125"><span><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/remote-sensing" title="Learn more about Remote Sensing">Remote sensing</a> provides tools can measure vegetation structure working across large areas (</span><a name="bbib18" href="#bib18" class="workspace-trigger">Kerr &amp; Ostrovsky, 2003</a><span><span>) and it plays an important role within the field of <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/forest-inventory" title="Learn more about Forest Inventory">forest inventory</a>. Airborne laser scanning (ALS) has also become an effective tool for acquiring forest inventory data. The </span><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a>
 of these field plots is typically carried out by means of differential 
global navigation satellite systems (DGNSS) and normally relies on 
logging times of 15–20&nbsp;min ensuring adequate accuracy under 
different forest conditions (</span><a name="bbib16" href="#bib16" class="workspace-trigger">Hauglin, Lien, Næsset, &amp; Gobakken, 2014</a>). Until recently, most remotely sensed data only provide only two-dimensional (2D) information (<a name="bbib24" href="#bib24" class="workspace-trigger">Seavy, Viers, &amp; Wood, 2009</a><span>)
 and information regarding the structure of vegetation, which requires 
3D to obtain for example tree heights, was has not been available. <a href="https://www.sciencedirect.com/topics/engineering/light-detection-and-ranging" title="Learn more about light detection and ranging">Light detection and ranging</a>
 (LIDAR) is a remote-sensing technique that can provide detailed 
information on vegetation structure. LIDAR imagery can be generated 
using low-flying aircraft to record the reflection data and measure the 
height of objects beneath the aircraft (</span><a href="http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full" target="_blank" rel="noreferrer noopener">http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full</a><a name="bbib19" href="#bib19" class="workspace-trigger">Lefsky, Cohen, Parker, &amp; Harding, 2002</a><span>). LIDAR imagery has been used to estimate the <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/canopy" title="Learn more about Canopy">canopy</a><span> height, stem diameter, canopy cover, <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/biomass" title="Learn more about Biomass">biomass</a> and other components of vegetation structure (</span></span><a href="http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full" target="_blank" rel="noreferrer noopener">http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full</a><a name="bbib4" href="#bib4" class="workspace-trigger">Chasmer, Hopkinson, &amp; Treitz, 2006</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full" target="_blank" rel="noreferrer noopener">http://onlinelibrary.wiley.com/doi/10.1890/08-1124.1/full</a><a name="bbib7" href="#bib7" class="workspace-trigger">Clawges, Vierling, Calhoon, &amp; Toomey, 2007</a><span>). However, since the spatial extent and <a href="https://www.sciencedirect.com/topics/engineering/spatial-resolution" title="Learn more about Spatial Resolution">spatial resolution</a> of a given sensor are inversely related (</span><a name="bbib13" href="#bib13" class="workspace-trigger">Franklin, Lavigne, Wulder, &amp; Stenhouse, 2002</a><span>) the vegetation structure <a href="https://www.sciencedirect.com/topics/engineering/characterisation" title="Learn more about Characterisation">characterisation</a> of large areas, based on remotely sensed data, often have sub-optimal precision for several applications (</span><a name="bbib30" href="#bib30" class="workspace-trigger">Xie, Sha, &amp; Yu, 2008</a>).</p><p id="p0130"><a name="bbib25" href="#bib25" class="workspace-trigger">Simonse, Aschoff, Spiecker, and Thies (2003)</a><span> used a terrestrial <a href="https://www.sciencedirect.com/topics/engineering/laser-scanner" title="Learn more about Laser Scanner">laser scanner</a>
 (TLS) for tree identification and to assess tree characteristics such 
as their positions and the diameters at breast height. This 
instrumentation presents a great accuracy of the data and the advantage 
of this technique is to obtain repeatable results of measurements 
because of the high level of automatism. However, the system was very 
expensive.</span></p><p id="p0135">This study proposes a smart, cost-effective, mobile stereovision system for in-field estimation of dendrometry parameters (<em>i.e.</em>,
 heights and diameters). The system includes a specifically developed 
mobile application for the management and georeferencing of the stereo 
images. The use of stereo images allows the measurement of the distance 
between two points using formulas for triangulation of the 3D 
coordinates and angles (<a name="bbib9" href="#bib9" class="workspace-trigger">Costa et&nbsp;al., 2009</a>, <a name="bbib20" href="#bib20" class="workspace-trigger">Menesatti et&nbsp;al., 2014</a>, <a name="bbib21" href="#bib21" class="workspace-trigger">Pallottino et&nbsp;al., 2015</a>, <a name="bbib29" href="#bib29" class="workspace-trigger">Wu et&nbsp;al., 2004</a>).</p></section><section id="sec2"><h2 id="sectitle0030" class="u-h3 u-margin-l-top u-margin-xs-bottom">2. Materials and methods</h2><p id="p0140">This study was carried out in 3 phases:<dl class="list"><dt class="list-label">−</dt><dd class="list-description"><p id="p0145"><span>training with <a href="https://www.sciencedirect.com/topics/engineering/calibration-system" title="Learn more about Calibration System">system calibration</a> through </span><a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a> analysis of known artificial objects;</p></dd><dt class="list-label">−</dt><dd class="list-description"><p id="p0150">testing with measurements of standing tree diameters and heights acquired through stereovision <a href="https://www.sciencedirect.com/topics/engineering/laser-system" title="Learn more about Laser System">system, laser</a><span> range finding (height) and tree <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/calipers" title="Learn more about Calipers">calliper</a> (diameter) and comparisons with direct measurements with felled trees;</span></p></dd><dt class="list-label">−</dt><dd class="list-description"><p id="p0155">field testing of application (in natural and urban forests using the stereovision system).</p></dd></dl></p><p id="p0160">In addition, during this last phase, an Android mobile application was developed for <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a> the stereo images and manage the additional information.</p><section id="sec2.1"><h3 id="sectitle0035" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Stereovision hardware and software</h3><div><p id="p0165">The
 stereovision system consists of two high resolution SLR cameras (Canon 
EOS 100D) mounted on plates, locked in a known position on a bar, 
provided with an anti-deformation rail system, and horizontal pan 
adjustment to obtain a greater precision and superior alignment between 
the two cameras. The tilt angle error for repeated camera positioning 
was &lt;0.001°. The two camera distances were adopted: 518&nbsp;mm used 
for the measurements of the trees diameters and 1479&nbsp;mm used for 
height measurement (<a name="bfig1" href="#fig1" class="workspace-trigger">Fig.&nbsp;1</a><span>).
 The exposure mode of the two cameras exposure was set to automatic mode
 and without flash use to allow the exposure to adapt to different <a href="https://www.sciencedirect.com/topics/engineering/lighting-condition" title="Learn more about Lighting Condition">lighting conditions</a>.
 During preliminary testing, the two stereo images were checked to 
verify that the exposure values were identical despite the slightly 
different scenes acquired.</span></p><figure class="figure text-xs" id="fig1"><span><img src="1-s2.jpg" alt="Fig.&nbsp;1" aria-describedby="cap0010" height="335"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (432KB)"><span class="anchor-text">Download high-res image (432KB)</span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0010"><p id="fspara0010"><span class="label">Fig.&nbsp;1</span>. <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">Stereovision</a>
 system consisting of two high resolution SLR cameras (Canon EOS 100D) 
mounted on plates positioned on stiff rails and with horizontal tilt 
adjustment.</p></span></div></figure></div><div><p id="p0170"><span>One of the main problems concerning stereovision regards the synchronism in the acquisition of the <a href="https://www.sciencedirect.com/topics/engineering/image-pair" title="Learn more about Image Pair">images pair</a><span>, especially for <a href="https://www.sciencedirect.com/topics/engineering/moving-object" title="Learn more about Moving Object">moving objects</a>.
 Therefore, was developed a device for synchronised shooting between the
 two cameras based on the open-source computer board Arduino (Board 
model NANO v3.3; </span></span><a name="bfig2" href="#fig2" class="workspace-trigger">Fig.&nbsp;2</a>). This device guarantees synchronism with a negligible margin of difference of about 35&nbsp;ms.</p><figure class="figure text-xs" id="fig2"><span><img src="1-s2_005.jpg" alt="Fig.&nbsp;2" aria-describedby="cap0015" height="441"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (237KB)"><span class="anchor-text">Download high-res image (237KB)</span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0015"><p id="fspara0015"><span class="label">Fig.&nbsp;2</span>. 
Device for simultaneous shooting control over the two high resolution 
SLR cameras (Canon EOS 100D) developed with an open source (Arduino) 
technology.</p></span></div></figure></div></section><section id="sec2.2"><h3 id="sectitle0040" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Data collection</h3><p id="p0175">The
 training phase consisted in measuring known distance on reference 
objects. Two kinds of objects were used: a T shape 3&nbsp;m&nbsp;bars 
with 250&nbsp;mm markers, and distances among landmarks measured on 
buildings (recorded with a laser meter; from 3 to 5&nbsp;m). A total of 
162, for the 1479&nbsp;mm camera distance, and of 384, for the 
518&nbsp;mm camera distance, measures were recorded to calibrate the 
system. Sensors were positioned at a distance from the object ranging 
from 1 to 30&nbsp;m.</p><p id="p0180"><span><span>Field testing was 
carried out at CREA in Monterotondo, Rome, Italy. The testing phase 
consists in measuring heights (poplars) and diameters (poplars, <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/limes" title="Learn more about Limes">limes</a>,
 firs, chestnut) of standing trees using the stereovision system, the 
traditional laser rangefinder (for heights) and the tree calliper (for 
diameters) on the same standing trees. Afterwards, these images were 
compared with the </span><a href="https://www.sciencedirect.com/topics/engineering/collected-data" title="Learn more about Collected Data">data collected</a><span> directly from felled trees. <a href="https://www.sciencedirect.com/topics/engineering/repeated-measurement" title="Learn more about Repeated Measurement">Repeated measurements</a>
 using different trained operators were carried out using the laser 
rangefinder and the tree calliper. A total of 194 tree heights and 50 
breast height tree diameters were collected. The reference measurements 
for both height and diameter were considered to be those from the felled
 trees. The “reference” diameters were obtained considering the mean 
diameter measured using at least 3 replicates using the tree calliper. 
Thus, the errors of the two systems used for height (laser rangefinder 
and stereovision), and the stereovision used for diameter, were 
calculated using the felled trees as reference using:</span></span><span class="display"><div id="fd1" class="formula"><span class="label">(1)</span><span class="math"><math><mrow is="true"><mo is="true">%</mo><mspace width="0.5em" is="true"></mspace><mi is="true">E</mi><mi is="true">r</mi><mi is="true">r</mi><mi is="true">o</mi><mi is="true">r</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">M</mi><mi is="true">e</mi><mi is="true">a</mi><mi is="true">s</mi><mi is="true">u</mi><mi is="true">r</mi><mi is="true">e</mi><mi is="true">m</mi><mi is="true">e</mi><mi is="true">n</mi><mi is="true">t</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">e</mi><mi is="true">f</mi><mi is="true">e</mi><mi is="true">r</mi><mi is="true">e</mi><mi is="true">n</mi><mi is="true">c</mi><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">R</mi><mi is="true">e</mi><mi is="true">f</mi><mi is="true">e</mi><mi is="true">r</mi><mi is="true">e</mi><mi is="true">n</mi><mi is="true">c</mi><mi is="true">e</mi></mrow></mfrac><mo is="true">·</mo><mn is="true">100</mn></mrow></math></span></div></span></p><p id="p0185">The operator error (<a name="bbib21" href="#bib21" class="workspace-trigger">Pallottino et&nbsp;al., 2015</a>)
 was evaluated by considering the variability in the measurements taken 
by different operators on the same tree, for laser rangefinder (height) 
and tree calliper (diameter), using:<span class="display"><div id="fd2" class="formula"><span class="label">(2)</span><span class="math"><math><mrow is="true"><mtext is="true">%</mtext><mspace width="0.5em" is="true"></mspace><mtext is="true">Operator</mtext><mspace width="0.5em" is="true"></mspace><mtext is="true">error</mtext><mspace width="0.25em" is="true"></mspace><mo is="true">=</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mtext is="true">j</mtext><mo is="true">=</mo><mn is="true">1</mn></mrow><mn is="true">3</mn></munderover><mrow is="true"><mrow is="true"><mo is="true">(</mo><mrow is="true"><mn is="true">1</mn><mi is="true" mathvariant="normal">−</mi><mfrac is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><msub is="true"><mi is="true">X</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">¯</mo></mover></mrow><mrow is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><msub is="true"><mi is="true">X</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">¯</mo></mover></mrow><mo is="true">+</mo><mi is="true">S</mi><msub is="true"><mi is="true">D</mi><mi is="true">j</mi></msub></mrow></mfrac></mrow><mo is="true">)</mo></mrow></mrow><mo is="true">·</mo><mn is="true">100</mn></mrow></math></span></div></span>where <span class="math"><math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><msub is="true"><mi is="true">X</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">¯</mo></mover></mrow></math></span> is the average repeated measurement of jth tree, SD<sub>j</sub> is the standard deviation of average repeated measures of jth tree and j is the number of trees with repeated measurements.</p><p id="p0190">During
 the field testing of the application a total of 194 image pairs were 
obtained using the stereovision system, of which 50 were from <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/woodlands" title="Learn more about Woodlands">woodland</a> near Serra San Bruno, Calabria, Italy and 144 were from urban parks in Rome, Italy.</p></section><section id="sec2.3"><h3 id="sectitle0045" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.3. Triangulation and measurement software</h3><p id="p0195">Specific
 software was designed to extract the distances between two points 
within stereo images through a triangulation method. The software, 
implemented in the Matlab (rel 7.1; Mathworks, Natick, Mass., USA) 
environment. A trained operator manually identified, on each stereo 
image, two points describing height or diameter (i.e. landmarks) and the
 software automatically returns the required measurement. The software 
could also extract of angles based on 3 landmarks on the two stereo 
images.</p></section><section id="sec2.4"><h3 id="sectitle0050" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.4. Georeferencing web application</h3><p id="p0200">An <em>ad hoc</em>
 georeferencing web application was developed for the stereo images data
 management and the introduction of geographic and additional 
information. The system, called infotracing (<a name="bbib8" href="#bib8" class="workspace-trigger">Costa et&nbsp;al., 2013</a><span><span><span>),
 consists of an application (designated SmartTree) developed for the 
Android OS platform. This application allows for image acquisition using
 a <a href="https://www.sciencedirect.com/topics/engineering/smartphone" title="Learn more about Smartphone">smartphone</a>
 and the storage of associated of additional information, including: 
information related to stereo images and GPS data for the position and 
species name of the tree. This information is stored on a centralised 
web database. The data on the database could be inserted or queried from
 different devices. Users could be technical (i.e. forestry) operators 
conducting </span><a href="https://www.sciencedirect.com/topics/engineering/field-operation" title="Learn more about Field Operation">field operations</a>,
 policy managers, inspectors, forest technicians and also generic users.
 The SmartTree application can work autonomously in an off-line mode, 
not synchronised with the web-database, or as a field digital </span><a href="https://www.sciencedirect.com/topics/engineering/data-collector" title="Learn more about Data Collector">data collector</a><span> for distributed surveys synchronised with the web-database. In this second mode, the application becomes the software <a href="https://www.sciencedirect.com/topics/engineering/interface-layer" title="Learn more about Interface Layer">interface layer</a> of a shared workflow established between the users to manage the data archived in the web database.</span></span></p><p id="p0205"><span>The
 web-database runs on a server which provides a web-site application 
(SmartTree-Web) to allow access from desktop terminals. The web 
application allows users to query and manage the collected data with the
 proper security policies. The application also provides specific 
geographic functionality for <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/data-analysis" title="Learn more about Data Analysis">data analysis</a>.
 The web application architecture is based on a classic client-server 
scheme and has been developed using open-source technology. The client 
application runs on a standard web browser and is implemented in 
JavaScript using Geo-Ext, an Ext-JS framework extension focused on 
geographical </span><a href="https://www.sciencedirect.com/topics/engineering/data-visualization" title="Learn more about Data Visualization">data visualisation</a><span><span><span>
 and editing. The client application has the appearance of a desktop 
application with tabular data, detail views and geographical map 
rendering. The client application does not <a href="https://www.sciencedirect.com/topics/engineering/data-store" title="Learn more about Data Store">store data</a> on the user's personal computer. All the data are provided by a </span><a href="https://www.sciencedirect.com/topics/engineering/application-server" title="Learn more about Application Server">server application</a>
 implemented in PHP and are stored in a web-database implemented in 
MySQL with a geographical extension. The data needed by the client 
application are requested by the server application to the database and 
are returned through the </span><a href="https://www.sciencedirect.com/topics/engineering/internet-connection" title="Learn more about Internet Connection">internet connection</a> to the client application.</span></p><p id="p0210"><span><span>The GPS information is acquired manually through the smartphone <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/antennae" title="Learn more about Antennae">antenna</a> with an accuracy approximately equal to 5&nbsp;m. Once the tree </span><a href="https://www.sciencedirect.com/topics/engineering/reference-image" title="Learn more about Reference Image">reference image</a><span>
 is acquired and stored, the operator needs to move as close as possible
 to the tree to acquire the tree coordinates. The GPS coordinates, 
relative to the stereo–images pair, are registered within the picture 
taken by the smartphone, according to the specifications of Exchangeable
 image file format (Exif), and consequently extracted. In time it is 
hoped that the GPS location accuracy will increase consistently due to 
the diffusion of L5 GNSS satellites characteristics and the capability 
of the smartphone chipset to lock first onto the satellite with the L1 
signal from satellites, and then to refine it with the L5. This is much 
less prone to distortions from <a href="https://www.sciencedirect.com/topics/engineering/multipath" title="Learn more about Multipath">multipath</a> reflections than L1 (</span></span><a name="bbib27" href="#bib27" class="workspace-trigger">Svaton, 2015</a>).</p><p id="p0215"><span>The
 acquired stereo images are archived and georeferenced creating a KML 
file for sharing and disseminating along with all the additional 
information manually inserted by the operator. The KML file was chosen 
as the <a href="https://www.sciencedirect.com/topics/engineering/interchanges" title="Learn more about Interchanges">interchange</a>
 file format because it has been adopted by various software platforms 
in order to eliminate problems while sharing information. Such files can
 be viewed through both, </span><a href="https://www.sciencedirect.com/topics/engineering/google-maps" title="Learn more about Google Maps">Google Maps</a>
 and Google Earth. The KML file was written using a dedicated utility in
 Java which elaborates the images acquired by the SmartTree application,
 including the extraction of all the information manually inserted by 
the operator. It then extracts the GPS position (latitude and longitude)
 from the smartphone JPG (Exif) creating a CSV file. This CSV file 
contains all the info of each single image with references to 
stereo-visual images acquired from SLR cameras. Subsequently the 
software generates the KML file containing: the single image, the 
position, the species and the notes, adapting them graphically according
 to the scheme of KML file.</p><p id="p0220">The KML file can be easily 
converted in a JSON structure and can be imported in the web-database to
 archive all the data collected by the application.</p><p id="p0225"><span>The application is not yet available via Google since it is a <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/prototypes" title="Learn more about Prototypes">prototype</a> developed in the specific context of the </span><a href="https://www.sciencedirect.com/topics/engineering/research-project" title="Learn more about Research Project">research projects</a> reported here.</p></section></section><section id="sec3"><h2 id="sectitle0055" class="u-h3 u-margin-l-top u-margin-xs-bottom">3. Results</h2><section id="sec3.1"><h3 id="sectitle0060" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.1. Training phase</h3><div><p id="p0230"><a name="bfig3" href="#fig3" class="workspace-trigger">Figure&nbsp;3</a><span>A shows a scatter plot for the average percentage error between the observed and estimated readings for the <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a><span><span>
 algorithm, during training performed on 162 objects of known 
measurement (from 100&nbsp;mm to 4500&nbsp;mm placed at distances from 5
 to 30&nbsp;m) with <a href="https://www.sciencedirect.com/topics/engineering/focal-length" title="Learn more about Focal Length">focal length</a> set at 18&nbsp;mm and </span><a href="https://www.sciencedirect.com/topics/engineering/distance-sensor" title="Learn more about Distance Sensor">sensor distance</a> was 1479&nbsp;mm. The average percentage error resulted was 1.76&nbsp;±&nbsp;1.33%. </span></span><a name="bfig3" href="#fig3" class="workspace-trigger">Figure&nbsp;3</a>B
 shows the same results but for the calibration performed on 384 objects
 and with a sensor distance equal to 518&nbsp;mm. For this phase the 
average percentage error was 0.93&nbsp;±&nbsp;1.12%. Since this distance
 between sensors is used only with small and close objects (i.e. trees 
diameters), this effect is negligible.</p><figure class="figure text-xs" id="fig3"><span><img src="1-s2_004.jpg" alt="Fig.&nbsp;3" aria-describedby="cap0020" height="270"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (143KB)"><span class="anchor-text">Download high-res image (143KB)</span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0020"><p id="fspara0020"><span class="label">Fig.&nbsp;3</span>. Scatter plots for the estimation of the average percentage error between the observed and estimated measurements by the <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a><span><span>
 algorithm, during training performed on 162 (A) and on 384 (B) objects 
of known measurement (from 100&nbsp;mm to 4500&nbsp;mm placed at 
distances from 5 to 30&nbsp;m) with <a href="https://www.sciencedirect.com/topics/engineering/focal-length" title="Learn more about Focal Length">focal length</a> set at 18&nbsp;mm and </span><a href="https://www.sciencedirect.com/topics/engineering/distance-sensor" title="Learn more about Distance Sensor">sensor distance</a> equal to 1479&nbsp;mm&nbsp;(A) and 518&nbsp;mm&nbsp;(B).</span></p></span></div></figure></div></section><section id="sec3.2"><h3 id="sectitle0065" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.2. Testing phase</h3><p id="p0235">The
 operator error (%) was 6.5&nbsp;±&nbsp;4.5% for the measurements of 
heights with laser rangefinder and to 1.8&nbsp;±&nbsp;1.7% for the 
measurements of diameters with the tree <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/calipers" title="Learn more about Calipers">calliper</a> for a total of 50 trees used for each.</p><div><p id="p0240"><a name="bfig4" href="#fig4" class="workspace-trigger">Figure&nbsp;4</a>
 shows the scatter plots of the differences between the reference 
measurements and these measured with the stereovisions in the testing 
phase. <a name="bfig4" href="#fig4" class="workspace-trigger">Figure&nbsp;4</a>A
 shows a scatter plot of the differences between the 50 observed heights
 (reference) and the heights measured with the laser rangefinder; the 
measurement error was equal to 6.8&nbsp;±&nbsp;6.6%. There were no 
significant differences between the two systems (t test 
p&nbsp;&gt;&nbsp;0.05). <a name="bfig4" href="#fig4" class="workspace-trigger">Figure&nbsp;4</a>B
 shows the scatter plot of the differences between the 50 observed 
heights (reference) and the heights measured with the stereovision. The 
measurement error was equal to 5.8&nbsp;±&nbsp;5.5%. There were no 
significant differences between the two systems (t test 
p&nbsp;&gt;&nbsp;0.05). <a name="bfig4" href="#fig4" class="workspace-trigger">Figure&nbsp;4</a>C
 shows the scatter plot of the differences between the 60 observed 
diameters (reference) and the diameters measured with the stereovision. 
The measurement error was equal to 4.2&nbsp;±&nbsp;3.0%. There were no 
significant differences between the two methodologies (t test 
p&nbsp;&gt;&nbsp;0.05).</p><figure class="figure text-xs" id="fig4"><span><img src="1-s2_002.jpg" alt="Fig.&nbsp;4" aria-describedby="cap0025" height="522"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (214KB)"><span class="anchor-text">Download high-res image (214KB)</span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0025"><p id="fspara0025"><span class="label">Fig.&nbsp;4</span>. 
Scatters plots of the testing phase. A) scatter plot of the differences 
between the 50 observed heights (reference) and the heights measured 
with the laser rangefinder; the measurement error was equal to 
6.8&nbsp;±&nbsp;6.6%; B) scatter plot of the differences between the 50 
observed heights (reference) and the heights measured with the <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a>.</p></span></div></figure></div></section><section id="sec3.3"><h3 id="sectitle0070" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.3. In-field testing phase</h3><div><p id="p0245"><a name="bfig5" href="#fig5" class="workspace-trigger">Figure&nbsp;5</a><span><span> shows screen images from the SmartTree application developed in this study for the <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a> and management of the stereo images developed in the </span><a href="https://www.sciencedirect.com/topics/engineering/research-project" title="Learn more about Research Project">research projects</a>
 reported here. Through this application, 50 surveys in Calabria and 144
 in Rome of georeferenced images were established with the stereovision 
system. The set of images were processed following capture and exported 
in KML format as reported earlier.</span></p><figure class="figure text-xs" id="fig5"><span><img src="1-s2_003.jpg" alt="Fig.&nbsp;5" aria-describedby="cap0030" height="329"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (655KB)"><span class="anchor-text">Download high-res image (655KB)</span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S1537511018303180-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0030"><p id="fspara0030"><span class="label">Fig.&nbsp;5</span>. Screen images from the SmartTree application developed in this study for <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a> and management of stereo images.</p></span></div></figure></div></section></section><section id="sec4"><h2 id="sectitle0075" class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Discussion</h2><p id="p0250"><span>Timely
 and accurate measurements of vegetation structure are increasingly 
needed across large areas to support a wide range of activities related 
to sustainable forest and urban <a href="https://www.sciencedirect.com/topics/engineering/green-space" title="Learn more about Green Space">green space</a> management (</span><a name="bbib23" href="#bib23" class="workspace-trigger">Rosenqvist, Milne, Lucas, Imhoff, &amp; Dobson, 2003</a><span>). This study deals with the setup of a smart <a href="https://www.sciencedirect.com/topics/engineering/stereovision" title="Learn more about Stereovision">stereovision</a><span> system for estimating dendrometry parameters (height and diameter), <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a>
 the relative information and the management of stereo images using a 
specifically developed application that is available in the field.</span></span></p><p id="p0255"><span><span>The
 stereovision system, being unable to process the data in real time, can
 optimise the data acquisition. The main limit to the use of the system 
is imposed by the visual field. It gives&nbsp;the possibility to 
estimate parameters without the need to fell the trees and the 
application can produce <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/biomass" title="Learn more about Biomass">biomass</a> estimates based on a single stereo </span><a href="https://www.sciencedirect.com/topics/engineering/image-pair" title="Learn more about Image Pair">images pair</a><span>. As previously mentioned, <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/remote-sensing" title="Learn more about Remote Sensing">remote sensing</a>
 based applications (such as LIDAR) represent a resource to measure 
vegetation structure across large areas. However, critical to the 
adoption of LIDAR as a survey tool, is the capacity to simultaneously 
measure in detail and with high accuracy both, vertical and horizontal 
vegetative structure as well as terrain morphology (</span></span><a name="bbib28" href="#bib28" class="workspace-trigger">Wulder et&nbsp;al., 2012</a><span>). In addition, because the spatial extent and <a href="https://www.sciencedirect.com/topics/engineering/spatial-resolution" title="Learn more about Spatial Resolution">spatial resolution</a> of a given sensor are inversely related (</span><a name="bbib13" href="#bib13" class="workspace-trigger">Franklin et&nbsp;al., 2002</a><span>) the <a href="https://www.sciencedirect.com/topics/engineering/characterisation" title="Learn more about Characterisation">characterisations</a> of large areas of vegetation using remote sensing tools often have sub-optimal precision for many applications (</span><a name="bbib30" href="#bib30" class="workspace-trigger">Xie et&nbsp;al., 2008</a>).</p><p id="p0260">The
 tools proposed here are particularly useful for urban green 
applications, given the possibility to georeference image acquisitions 
through the mobile device and insert additional information such as, for
 example, <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/tree-health" title="Learn more about Tree Health">tree health</a>
 status. Moreover, the stereovision system has the possibility of 
totally surveying the plant, and of measuring the distances and angles, 
and this could be used to analyse and classify the state of mechanical 
danger represented by each tree toppling or otherwise failing.</p><p id="p0265"><span>In
 forests, acquisition of multiple dendrometry parameters is needed, and,
 even if it becomes difficult to frame the entire plant (information on 
the height of the individual), the system can be useful to support 
measurements of <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/basal-area" title="Learn more about Basal Area">basal area</a> using the relascope (</span><a name="bbib2" href="#bib2" class="workspace-trigger">Bitterlich, 1984</a><span><span>). In fact, from the readings of the diameters it is possible to derive other population statistics such as <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/population-size" title="Learn more about Population Size">population size</a> and the diameter </span><a href="https://www.sciencedirect.com/topics/engineering/class-distribution" title="Learn more about Class Distribution">class distribution</a><span>. With repeated measures over time, accurate diameter estimation provided by <a href="https://www.sciencedirect.com/topics/engineering/stereo-vision" title="Learn more about Stereo Vision">stereo-vision</a><span>
 could allow other important parameters related to the timber growth to 
be determined. Moreover, using the same information integrating the 
dendrometry measurements and the <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/leaf-area-index" title="Learn more about Leaf Area Index">leaf area index</a> (LAI; </span></span></span><a name="bbib6" href="#bib6" class="workspace-trigger">Chianucci, Puletti, Giacomello, Cutini, &amp; Corona, 2015</a>)
 and knowing the age of the tree population it could be possible to 
estimate the health status of the trees and establish the level of soil 
fertility. The same technology could be further applied for the 
evaluation of forestry vehicles transit inside wooded areas.</p><p id="p0270"><span>The
 “user friendly” information provided could be used by a field 
operative, as well as by a trained technician. In fact, a member of the 
public with this application, could both actively provide information as
 well as receive it. In this way, a <a href="https://www.sciencedirect.com/topics/engineering/public-institution" title="Learn more about Public Institution">public institution</a><span><span><span>
 would already have a data base of trees at risk of failure to 
commission a technician to perform a visual tree assessment (VTA) 
analysis. The use of such technology could improve <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/profitability" title="Learn more about Profitability">profitability</a> and help </span><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/foresters" title="Learn more about Foresters">foresters</a> make economic and </span><a href="https://www.sciencedirect.com/topics/engineering/environmental-management" title="Learn more about Environmental Management">environmental management</a><span> decisions for treatment of individual trees and forest stands, improve thinning and <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/harvesting" title="Learn more about Harvesting">harvesting</a> operations and efficiently allocating timber resources for optimal utilisation (</span></span></span><a name="bbib22" href="#bib22" class="workspace-trigger">Proto et&nbsp;al., 2017</a>).</p></section><section id="sec5"><h2 id="sectitle0080" class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Conclusions</h2><p id="p0275"><span><span>The
 proposed system allows dendrometry information to be acquired with an 
error comparable to the one of the devices used. The information can be 
georeferenced with using web-based application (SmartTree). The system 
can be considered as a valid tool but with some limitations due, for 
example, to the poor <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/woodlands" title="Learn more about Woodlands">woodland</a> management, low </span><a href="https://www.sciencedirect.com/topics/engineering/usability" title="Learn more about Usability">usability</a><span><span>
 in coppices and the impossibility of estimating heights in the presence
 of dense vegetation. Because the two cameras were set to the automatic 
exposure mode, the resulting images were adapted to varying <a href="https://www.sciencedirect.com/topics/engineering/lighting-condition" title="Learn more about Lighting Condition">lighting conditions</a> acquired. However, the possibility of optimising the </span><a href="https://www.sciencedirect.com/topics/engineering/acquisition-time" title="Learn more about Acquisition Time">acquisition times</a>
 in the forest, for example by the estimation of diameters, allows the 
number of observations to be increase and giving better approximations 
than traditionally methods using </span></span><a href="https://www.sciencedirect.com/topics/engineering/manual-measurement" title="Learn more about Manual Measurement">manual measurement</a><span>
 that are also subject errors or to the relascope. Finally, the 
application of the proposed technology has the possibility to use the 
dendrometry data acquired by the <a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/georeferencing" title="Learn more about Georeferencing">georeferencing</a> and the positioning of the forest plants with the ultimate aim of geolocating the wood mass present in the examined area.</span></p></section></div><section id="ack0010"><h2 id="sectitle0085" class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgements</h2><p id="p0280">Some activities in this study were funded by the <span id="gs1">Italian Ministry of University and scientific research (MIUR)</span>, as part of the project “<a href="#gs1">Ambi.tec.Fil.Legno” (PON03PE_00024_1)</a> and by the project <a href="#gs2">AGROENER (D.D. n. 26329)</a> funded by the <span id="gs2">Italian Ministry of Agriculture (MiPAAF)</span>; some other has been funded by <span id="gs3">Regione Lazio FILAS</span>, project <a href="#gs3">FILAS URBANFOR3 (d. n. 270/DG)</a><span>.
 Authors would like to thank Matteo Viscardi and Tullio Guglielmelli of 
University of Tuscia of Viterbo, and the personnel of CREA-IT Dr. 
Marcello Biocca, Dr. Andrea Acampora, Dr. Giuseppina Di Loreto, and for 
their support in the <a href="https://www.sciencedirect.com/topics/engineering/field-operation" title="Learn more about Field Operation">field operations</a> Sandu Lazar and Franco Bernardini. All the authors equally contribute to the writing of the paper and to its content.</span></p></section></div><div class="related-content-links u-hide-from-md"><button class="button button-anchor" type="button"><span class="button-text">Recommended articles</span></button><button class="button button-anchor" type="button"><span class="button-text">Citing articles (1)</span></button></div><div class="Tail"></div><section class="bibliography u-font-serif text-s" id="cebib0010"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="cebibsec0010"><dl class="references" id="reference-links-cebibsec0010"><dt class="label"><a href="#bbib1" id="ref-id-bib1">Bai et&nbsp;al., 2005</a></dt><dd class="reference" id="sref1"><div class="contribution">X.B. Bai, K.Q. Wang, H. Wang<strong class="title">Research on the classification of wood texture based on Gray Level Co-occurrence Matrix</strong></div><div class="host">Journal of Harbin Institute of Technology, 12 (2005), p. 021</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Research%20on%20the%20classification%20of%20wood%20texture%20based%20on%20Gray%20Level%20Co-occurrence%20Matrix&amp;publication_year=2005&amp;author=X.B.%20Bai&amp;author=K.Q.%20Wang&amp;author=H.%20Wang">Google Scholar</a></div></dd><dt class="label"><a href="#bbib2" id="ref-id-bib2">Bitterlich, 1984</a></dt><dd class="reference" id="sref2"><div class="contribution">W. Bitterlich<strong class="title">The relascope idea: Relative measurements in forestry</strong></div><div class="host">Commonwealth Agricultural Bureaux, Farnham Royal (Slough). England (1984)</div><div class="comment">242 pp.</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20relascope%20idea%3A%20Relative%20measurements%20in%20forestry&amp;publication_year=1984&amp;author=W.%20Bitterlich">Google Scholar</a></div></dd><dt class="label"><a href="#bbib3" id="ref-id-bib3">Brownlie et&nbsp;al., 2007</a></dt><dd class="reference" id="sref3"><div class="contribution">R.K. Brownlie, W.W. Carson, J.G. Firth, C.J. Goulding<strong class="title">Image-based dendrometry system for standing trees</strong></div><div class="host">New Zealand Journal of Forestry Science, 37 (2) (2007), p. 153</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34547754446&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Image-based%20dendrometry%20system%20for%20standing%20trees&amp;publication_year=2007&amp;author=R.K.%20Brownlie&amp;author=W.W.%20Carson&amp;author=J.G.%20Firth&amp;author=C.J.%20Goulding">Google Scholar</a></div></dd><dt class="label"><a href="#bbib4" id="ref-id-bib4">Chasmer et&nbsp;al., 2006</a></dt><dd class="reference" id="sref4"><div class="contribution">L. Chasmer, C. Hopkinson, P. Treitz<strong class="title">Investigating laser pulse penetration through a conifer canopy by integrating airborne and terrestrial lidar data</strong></div><div class="host">Canadian Journal of Remote Sensing, 32 (2006), pp. 116-125</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.5589/m06-011">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33746142045&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Investigating%20laser%20pulse%20penetration%20through%20a%20conifer%20canopy%20by%20integrating%20airborne%20and%20terrestrial%20lidar%20data&amp;publication_year=2006&amp;author=L.%20Chasmer&amp;author=C.%20Hopkinson&amp;author=P.%20Treitz">Google Scholar</a></div></dd><dt class="label"><a href="#bbib5" id="ref-id-bib5">Chianucci and Cutini, 2013</a></dt><dd class="reference" id="sref5"><div class="contribution">F. Chianucci, A. Cutini<strong class="title">Estimation of canopy properties in deciduous forests with digital hemispherical and cover photography</strong></div><div class="host">Agricultural and Forest Meteorology, 168 (2013), pp. 130-139</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0168192312002766" aria-describedby="ref-id-sref5">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0168192312002766/pdfft?md5=2722fd27399a21f4b50f5511710b610a&amp;pid=1-s2.0-S0168192312002766-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866996325&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Estimation%20of%20canopy%20properties%20in%20deciduous%20forests%20with%20digital%20hemispherical%20and%20cover%20photography&amp;publication_year=2013&amp;author=F.%20Chianucci&amp;author=A.%20Cutini">Google Scholar</a></div></dd><dt class="label"><a href="#bbib6" id="ref-id-bib6">Chianucci et&nbsp;al., 2015</a></dt><dd class="reference" id="sref6"><div class="contribution">F. Chianucci, N. Puletti, E. Giacomello, A. Cutini, P. Corona<strong class="title">Estimation of leaf area index in isolated trees with digital photography and its application to urban forestry</strong></div><div class="host">Urban Forestry and Urban Greening, 14 (2) (2015), pp. 377-382</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S1618866715000412" aria-describedby="ref-id-sref6">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S1618866715000412/pdfft?md5=1ed51a814ad176575eda51cb9ea17dd4&amp;pid=1-s2.0-S1618866715000412-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84940165172&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Estimation%20of%20leaf%20area%20index%20in%20isolated%20trees%20with%20digital%20photography%20and%20its%20application%20to%20urban%20forestry&amp;publication_year=2015&amp;author=F.%20Chianucci&amp;author=N.%20Puletti&amp;author=E.%20Giacomello&amp;author=A.%20Cutini&amp;author=P.%20Corona">Google Scholar</a></div></dd><dt class="label"><a href="#bbib7" id="ref-id-bib7">Clawges et&nbsp;al., 2007</a></dt><dd class="reference" id="sref7"><div class="contribution">R. Clawges, L. Vierling, M. Calhoon, M. Toomey<strong class="title">Use of a ground-based scanning lidar for estimation of biophysical properties of western larch (<em>Larix occidentalis</em>)</strong></div><div class="host">International Journal of Remote Sensing, 28 (2007), pp. 4331-4344</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/01431160701243460">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34748854853&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Use%20of%20a%20ground-based%20scanning%20lidar%20for%20estimation%20of%20biophysical%20properties%20of%20western%20larch%20%28Larix%20occidentalis%29&amp;publication_year=2007&amp;author=R.%20Clawges&amp;author=L.%20Vierling&amp;author=M.%20Calhoon&amp;author=M.%20Toomey">Google Scholar</a></div></dd><dt class="label"><a href="#bbib8" id="ref-id-bib8">Costa et&nbsp;al., 2013</a></dt><dd class="reference" id="sref8"><div class="contribution">C. Costa, F. Antonucci, F. Pallottino, J. Aguzzi, D. Sarri, P. Menesatti<strong class="title">A review on agri-food supply chain traceability by means of RFID technology</strong></div><div class="host">Food and Bioprocess Technology, 6 (2013), pp. 353-366</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/s11947-012-0958-7">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84872605720&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20review%20on%20agri-food%20supply%20chain%20traceability%20by%20means%20of%20RFID%20technology&amp;publication_year=2013&amp;author=C.%20Costa&amp;author=F.%20Antonucci&amp;author=F.%20Pallottino&amp;author=J.%20Aguzzi&amp;author=D.%20Sarri&amp;author=P.%20Menesatti">Google Scholar</a></div></dd><dt class="label"><a href="#bbib9" id="ref-id-bib9">Costa et&nbsp;al., 2009</a></dt><dd class="reference" id="sref9"><div class="contribution">C. Costa, M. Scardi, V. Vitalini, S. Cataudella<strong class="title">A dual camera system for counting and sizing Northern Bluefin Tuna (<em>Thunnus thynnus</em>; Linnaeus, 1758) stock, during transfer to aquaculture cages, with a semi automatic Artificial Neural Network tool</strong></div><div class="host">Aquaculture, 291 (3–4) (2009), pp. 161-167</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0044848609001082" aria-describedby="ref-id-sref9">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0044848609001082/pdfft?md5=019c204c2fa24984cb98655d8a939637&amp;pid=1-s2.0-S0044848609001082-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67349171901&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20dual%20camera%20system%20for%20counting%20and%20sizing%20Northern%20Bluefin%20Tuna%20%28Thunnus%20thynnus%3B%20Linnaeus%2C%201758%29%20stock%2C%20during%20transfer%20to%20aquaculture%20cages%2C%20with%20a%20semi%20automatic%20Artificial%20Neural%20Network%20tool&amp;publication_year=2009&amp;author=C.%20Costa&amp;author=M.%20Scardi&amp;author=V.%20Vitalini&amp;author=S.%20Cataudella">Google Scholar</a></div></dd><dt class="label"><a href="#bbib10" id="ref-id-bib10">Dahl et&nbsp;al., 2006</a></dt><dd class="reference" id="sref10"><div class="contribution">A.B. Dahl, M. Guo, K.H. Madsen<strong class="title">Scale-space and watershed segmentation for detection of wood logs</strong></div><div class="host">Vision day, informatics and mathematical modelling (2006)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Scale-space%20and%20watershed%20segmentation%20for%20detection%20of%20wood%20logs&amp;publication_year=2006&amp;author=A.B.%20Dahl&amp;author=M.%20Guo&amp;author=K.H.%20Madsen">Google Scholar</a></div></dd><dt class="label"><a href="#bbib11" id="ref-id-bib11">Fink, 2004</a></dt><dd class="reference" id="sref11"><div class="contribution">F. Fink<strong class="title">Foto-optische erfassung der dimension von nadelrundholzabschnitten unter einsatz digitaler bildverarbeitender methoden</strong></div><div class="comment">Dissertation</div><div class="host">Fakultaet fuer Forst- und Umweltwissenschaften der Albert-Ludwigs-Universitaet Freiburg i. Brsg (2004)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Foto-optische%20erfassung%20der%20dimension%20von%20nadelrundholzabschnitten%20unter%20einsatz%20digitaler%20bildverarbeitender%20methoden&amp;publication_year=2004&amp;author=F.%20Fink">Google Scholar</a></div></dd><dt class="label"><a href="#bbib12" id="ref-id-bib12">Firth et&nbsp;al., 2000</a></dt><dd class="reference" id="sref12"><div class="contribution">J.G. Firth, R.K. Brownlie, W.W. Carson<strong class="title">Accurate stem measurements key to new image-based system</strong></div><div class="host">New Zealand Journal of Forestry, 45 (2000), pp. 25-29</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034239727&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Accurate%20stem%20measurements%20key%20to%20new%20image-based%20system&amp;publication_year=2000&amp;author=J.G.%20Firth&amp;author=R.K.%20Brownlie&amp;author=W.W.%20Carson">Google Scholar</a></div></dd><dt class="label"><a href="#bbib13" id="ref-id-bib13">Franklin et&nbsp;al., 2002</a></dt><dd class="reference" id="sref13"><div class="contribution">S.E. Franklin, M.B. Lavigne, M.A. Wulder, G.B. Stenhouse<strong class="title">Change detection and landscape structure mapping using remote sensing</strong></div><div class="host">The Forestry Chronicle, 78 (2002), pp. 618-625</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.5558/tfc78618-5">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036766771&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Change%20detection%20and%20landscape%20structure%20mapping%20using%20remote%20sensing&amp;publication_year=2002&amp;author=S.E.%20Franklin&amp;author=M.B.%20Lavigne&amp;author=M.A.%20Wulder&amp;author=G.B.%20Stenhouse">Google Scholar</a></div></dd><dt class="label"><a href="#bbib14" id="ref-id-bib14">Gobakken and Næsset, 2004</a></dt><dd class="reference" id="sref14"><div class="contribution">T. Gobakken, E. Næsset<strong class="title">Estimation of diameter and basal area distributions in coniferous forest by means of airborne laser scanner data</strong></div><div class="host">Scandinavian Journal of Forest Research, 19 (6) (2004), pp. 529-542</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/02827580410019454">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-11144304495&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Estimation%20of%20diameter%20and%20basal%20area%20distributions%20in%20coniferous%20forest%20by%20means%20of%20airborne%20laser%20scanner%20data&amp;publication_year=2004&amp;author=T.%20Gobakken&amp;author=E.%20N%C3%A6sset">Google Scholar</a></div></dd><dt class="label"><a href="#bbib15" id="ref-id-bib15">Gutzeit and Voskamp, 2012</a></dt><dd class="reference" id="sref15"><div class="contribution">E. Gutzeit, J. Voskamp<strong class="title">Automatic segmentation of wood logs by combining detection and segmentation</strong></div><div class="host">International symposium on visual computing, Springer, Berlin, Heidelberg (2012), pp. 252-261</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/978-3-642-33179-4_25">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866714183&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Automatic%20segmentation%20of%20wood%20logs%20by%20combining%20detection%20and%20segmentation&amp;publication_year=2012&amp;author=E.%20Gutzeit&amp;author=J.%20Voskamp">Google Scholar</a></div></dd><dt class="label"><a href="#bbib16" id="ref-id-bib16">Hauglin et&nbsp;al., 2014</a></dt><dd class="reference" id="sref16"><div class="contribution">M. Hauglin, V. Lien, E. Næsset, T. Gobakken<strong class="title">Geo-referencing forest field plots by co-registration of terrestrial and airborne laser scanning data</strong></div><div class="host">International Journal of Remote Sensing, 35 (9) (2014), pp. 3135-3149</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/01431161.2014.903440">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84899643762&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Geo-referencing%20forest%20field%20plots%20by%20co-registration%20of%20terrestrial%20and%20airborne%20laser%20scanning%20data&amp;publication_year=2014&amp;author=M.%20Hauglin&amp;author=V.%20Lien&amp;author=E.%20N%C3%A6sset&amp;author=T.%20Gobakken">Google Scholar</a></div></dd><dt class="label"><a href="#bbib17" id="ref-id-bib17">Herrera et&nbsp;al., 2011</a></dt><dd class="reference" id="sref17"><div class="contribution">P.J. Herrera, G. Pajares, M. Guijarro, J.J. Ruz, J.M. Cruz<strong class="title">A stereovision matching strategy for images captured with fish-eye lenses in forest environments</strong></div><div class="host">Sensors, 11 (2) (2011), pp. 1756-1783</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.3390/s110201756">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79952090472&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20stereovision%20matching%20strategy%20for%20images%20captured%20with%20fish-eye%20lenses%20in%20forest%20environments&amp;publication_year=2011&amp;author=P.J.%20Herrera&amp;author=G.%20Pajares&amp;author=M.%20Guijarro&amp;author=J.J.%20Ruz&amp;author=J.M.%20Cruz">Google Scholar</a></div></dd><dt class="label"><a href="#bbib18" id="ref-id-bib18">Kerr and Ostrovsky, 2003</a></dt><dd class="reference" id="sref18"><div class="contribution">J.T. Kerr, M. Ostrovsky<strong class="title">Fromspace to species: Ecological applications for remote sensing</strong></div><div class="host">Trends in Ecology &amp; Evolution, 18 (2003), pp. 299-314</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037500103&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Fromspace%20to%20species%3A%20Ecological%20applications%20for%20remote%20sensing&amp;publication_year=2003&amp;author=J.T.%20Kerr&amp;author=M.%20Ostrovsky">Google Scholar</a></div></dd><dt class="label"><a href="#bbib19" id="ref-id-bib19">Lefsky et&nbsp;al., 2002</a></dt><dd class="reference" id="sref19"><div class="contribution">M.A. Lefsky, W.B. Cohen, G.G. Parker, D.J. Harding<strong class="title">Lidar remote sensing for ecosystem studies</strong></div><div class="host">BioScience, 52 (2002), pp. 19-30</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1641/0006-3568%282002%29052[0019:LRSFES]2.0.CO;2">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036165976&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Lidar%20remote%20sensing%20for%20ecosystem%20studies&amp;publication_year=2002&amp;author=M.A.%20Lefsky&amp;author=W.B.%20Cohen&amp;author=G.G.%20Parker&amp;author=D.J.%20Harding">Google Scholar</a></div></dd><dt class="label"><a href="#bbib20" id="ref-id-bib20">Menesatti et&nbsp;al., 2014</a></dt><dd class="reference" id="sref20"><div class="contribution">P. Menesatti, C. Costa, F. Antonucci, R. Steri, F. Pallottino, G. Catillo<strong class="title">A low-cost stereovision system to estimate size and weight of live sheep</strong></div><div class="host">Computers and Electronics in Agriculture, 103 (2014), pp. 33-38</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0168169914000362" aria-describedby="ref-id-sref20">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0168169914000362/pdfft?md5=42b8a6981b43e8e5ac3f96eec271df61&amp;pid=1-s2.0-S0168169914000362-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84896878388&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20low-cost%20stereovision%20system%20to%20estimate%20size%20and%20weight%20of%20live%20sheep&amp;publication_year=2014&amp;author=P.%20Menesatti&amp;author=C.%20Costa&amp;author=F.%20Antonucci&amp;author=R.%20Steri&amp;author=F.%20Pallottino&amp;author=G.%20Catillo">Google Scholar</a></div></dd><dt class="label"><a href="#bbib21" id="ref-id-bib21">Pallottino et&nbsp;al., 2015</a></dt><dd class="reference" id="sref21"><div class="contribution">F. Pallottino, R. Steri, P. Menesatti, F. Antonucci, C. Costa, S. Figorilli, <em> et al.</em><strong class="title">Comparison between manual and stereovision body traits measurements of Lipizzan horses</strong></div><div class="host">Computers and Electronics in Agriculture, 118 (2015), pp. 408-413</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S016816991500294X" aria-describedby="ref-id-sref21">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S016816991500294X/pdfft?md5=2523cb0200f8e245b09475c008da22ff&amp;pid=1-s2.0-S016816991500294X-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84944064728&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Comparison%20between%20manual%20and%20stereovision%20body%20traits%20measurements%20of%20Lipizzan%20horses&amp;publication_year=2015&amp;author=F.%20Pallottino&amp;author=R.%20Steri&amp;author=P.%20Menesatti&amp;author=F.%20Antonucci&amp;author=C.%20Costa&amp;author=S.%20Figorilli">Google Scholar</a></div></dd><dt class="label"><a href="#bbib22" id="ref-id-bib22">Proto et&nbsp;al., 2017</a></dt><dd class="reference" id="sref22"><div class="contribution">A.R. Proto, G. Macrì, V. Bernardini, D. Russo, G. Zimbalatti<strong class="title">Acoustic evaluation of wood quality with a non-destructive method in standing trees: A first survey in Italy</strong></div><div class="host">iForest: Biogeosciences and Forestry, 10 (4) (2017), pp. 700-706</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.3832/ifor2065-010">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85025153783&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Acoustic%20evaluation%20of%20wood%20quality%20with%20a%20non-destructive%20method%20in%20standing%20trees%3A%20A%20first%20survey%20in%20Italy&amp;publication_year=2017&amp;author=A.R.%20Proto&amp;author=G.%20Macr%C3%AC&amp;author=V.%20Bernardini&amp;author=D.%20Russo&amp;author=G.%20Zimbalatti">Google Scholar</a></div></dd><dt class="label"><a href="#bbib23" id="ref-id-bib23">Rosenqvist et&nbsp;al., 2003</a></dt><dd class="reference" id="sref23"><div class="contribution">A. Rosenqvist, A. Milne, R. Lucas, M. Imhoff, C. Dobson<strong class="title">A review of remote sensing technology in support of the Kyoto protocol</strong></div><div class="host">Environmental Science &amp; Policy, 6 (2003), pp. 441-455</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S1462901103000704" aria-describedby="ref-id-sref23">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S1462901103000704/pdfft?md5=89bf1c7139749a35c358b8b02f0746b8&amp;pid=1-s2.0-S1462901103000704-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0042890290&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20review%20of%20remote%20sensing%20technology%20in%20support%20of%20the%20Kyoto%20protocol&amp;publication_year=2003&amp;author=A.%20Rosenqvist&amp;author=A.%20Milne&amp;author=R.%20Lucas&amp;author=M.%20Imhoff&amp;author=C.%20Dobson">Google Scholar</a></div></dd><dt class="label"><a href="#bbib24" id="ref-id-bib24">Seavy et&nbsp;al., 2009</a></dt><dd class="reference" id="sref24"><div class="contribution">N.E. Seavy, J.H. Viers, J.K. Wood<strong class="title">Riparian bird response to vegetation structure: A multiscale analysis using LiDAR measurements of canopy height</strong></div><div class="host">Ecological Applications, 19 (7) (2009), pp. 1848-1857</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1890/08-1124.1">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70349336779&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Riparian%20bird%20response%20to%20vegetation%20structure%3A%20A%20multiscale%20analysis%20using%20LiDAR%20measurements%20of%20canopy%20height&amp;publication_year=2009&amp;author=N.E.%20Seavy&amp;author=J.H.%20Viers&amp;author=J.K.%20Wood">Google Scholar</a></div></dd><dt class="label"><a href="#bbib25" id="ref-id-bib25">Simonse et&nbsp;al., 2003</a></dt><dd class="reference" id="sref25"><div class="contribution">M. Simonse, T. Aschoff, H. Spiecker, M. Thies<strong class="title">Automatic determination of forest inventory parameters using terrestrial laser scanning</strong></div><div class="host">Proceedings of the scandlaser scientific workshop on airborne laser scanning of forests, Vol. 2003 (2003), pp. 252-258</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85050204626&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Automatic%20determination%20of%20forest%20inventory%20parameters%20using%20terrestrial%20laser%20scanning&amp;publication_year=2003&amp;author=M.%20Simonse&amp;author=T.%20Aschoff&amp;author=H.%20Spiecker&amp;author=M.%20Thies">Google Scholar</a></div></dd><dt class="label"><a href="#bbib26" id="ref-id-bib26">Smiley et&nbsp;al., 2007</a></dt><dd class="reference" id="sref26"><div class="contribution">E.T. Smiley, B.R. Fraedrich, P. Fengler<strong class="title">Hazard tree inspection, evaluation, and management</strong></div><div class="host">Urban and community forestry in the Northeast, Springer, Dordrecht (2007), pp. 277-294</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/978-1-4020-4289-8_17">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Hazard%20tree%20inspection%2C%20evaluation%2C%20and%20management&amp;publication_year=2007&amp;author=E.T.%20Smiley&amp;author=B.R.%20Fraedrich&amp;author=P.%20Fengler">Google Scholar</a></div></dd><dt class="label"><a href="#bbib27" id="ref-id-bib27">Svaton, 2015</a></dt><dd class="reference" id="sref27"><div class="contribution">J. Svaton<strong class="title">Experimental reception of new GNSS signals</strong></div><div class="host">2015 International Association of Institutes of Navigation World Congress (IAIN) (2015), pp. 1-4</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/IAIN.2015.7352223">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Experimental%20reception%20of%20new%20GNSS%20signals&amp;publication_year=2015&amp;author=J.%20Svaton">Google Scholar</a></div></dd><dt class="label"><a href="#bbib28" id="ref-id-bib28">Wulder et&nbsp;al., 2012</a></dt><dd class="reference" id="sref28"><div class="contribution">M.A. Wulder, J.C. White, R.F. Nelson, E. Næsset, H.O. Ørka, N.C. Coops, <em> et al.</em><strong class="title">Lidar sampling for large-area forest characterization: A review</strong></div><div class="host">Remote Sensing of Environment, 121 (2012), pp. 196-209</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0034425712000855" aria-describedby="ref-id-sref28">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0034425712000855/pdfft?md5=585ef139d12e5e7a472587546fc3c85a&amp;pid=1-s2.0-S0034425712000855-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84863421727&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Lidar%20sampling%20for%20large-area%20forest%20characterization%3A%20A%20review&amp;publication_year=2012&amp;author=M.A.%20Wulder&amp;author=J.C.%20White&amp;author=R.F.%20Nelson&amp;author=E.%20N%C3%A6sset&amp;author=H.O.%20%C3%98rka&amp;author=N.C.%20Coops">Google Scholar</a></div></dd><dt class="label"><a href="#bbib29" id="ref-id-bib29">Wu et&nbsp;al., 2004</a></dt><dd class="reference" id="sref29"><div class="contribution">J. Wu, R. Tillett, N. McFarlane, X. Ju, J.P. Siebert, P. Schofield<strong class="title">Extracting the three-dimensional shape of live pigs using stereo photogrammetry</strong></div><div class="host">Computers and Electronics in Agriculture, 44 (2004), pp. 203-222</div><div class="ReferenceLinks u-font-sans"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0168169904000808" aria-describedby="ref-id-sref29">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0168169904000808/pdfft?md5=c69e3c9e49184232fad786a50204e4af&amp;pid=1-s2.0-S0168169904000808-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-4143077870&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Extracting%20the%20three-dimensional%20shape%20of%20live%20pigs%20using%20stereo%20photogrammetry&amp;publication_year=2004&amp;author=J.%20Wu&amp;author=R.%20Tillett&amp;author=N.%20McFarlane&amp;author=X.%20Ju&amp;author=J.P.%20Siebert&amp;author=P.%20Schofield">Google Scholar</a></div></dd><dt class="label"><a href="#bbib30" id="ref-id-bib30">Xie et&nbsp;al., 2008</a></dt><dd class="reference" id="sref30"><div class="contribution">Y. Xie, Z. Sha, M. Yu<strong class="title">Remote sensing imagery in vegetation mapping: A review</strong></div><div class="host">Journal of Plant Ecology, 1 (2008), pp. 9-23</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1093/jpe/rtm005">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77549088874&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Remote%20sensing%20imagery%20in%20vegetation%20mapping%3A%20A%20review&amp;publication_year=2008&amp;author=Y.%20Xie&amp;author=Z.%20Sha&amp;author=M.%20Yu">Google Scholar</a></div></dd></dl></section></section><div class="Copyright"><span class="copyright-line">© 2018 IAgrE. Published by Elsevier Ltd. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class="RelatedContent" aria-label="Related content"><section class="SidePanel u-margin-s-bottom"><header id="recommended-articles-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Recommended articles</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S1537511017310723"><h3 class="article-title ellipsis text-s" id="recommended-articles-article0-title" title="A photogrammetry-based image registration method for multi-camera systems – With applications in images of a tree crop"><span>A photogrammetry-based image registration method for multi-camera systems – With applications in images of a tree crop</span></h3></a><div class="article-source ellipsis"><div class="source">Biosystems Engineering, Volume 174, 2018, pp. 89-106</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S1537511017310723/pdfft?md5=168dc43738606717dc38ec6db01fec99&amp;pid=1-s2.0-S1537511017310723-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" aria-describedby="recommended-articles-article0-title" aria-controls="recommended-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S1537511017305925"><h3 class="article-title ellipsis text-s" id="recommended-articles-article1-title" title="Bayberry image segmentation based on manifold ranking salient object detection method"><span>Bayberry image segmentation based on manifold ranking salient object detection method</span></h3></a><div class="article-source ellipsis"><div class="source">Biosystems Engineering, Volume 178, 2019, pp. 264-274</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S1537511017305925/pdfft?md5=2d84df97bf314973af567db82f91293a&amp;pid=1-s2.0-S1537511017305925-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" aria-describedby="recommended-articles-article1-title" aria-controls="recommended-articles-article1" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article1" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S1537511016308777"><h3 class="article-title ellipsis text-s" id="recommended-articles-article2-title" title="Transfer learning for the classification of sugar beet and volunteer potato under field conditions"><span>Transfer learning for the classification of sugar beet and volunteer potato under field conditions</span></h3></a><div class="article-source ellipsis"><div class="source">Biosystems Engineering, Volume 174, 2018, pp. 50-65</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S1537511016308777/pdfft?md5=f038e89ac982ba5588f7fb639a7480ca&amp;pid=1-s2.0-S1537511016308777-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" aria-describedby="recommended-articles-article2-title" aria-controls="recommended-articles-article2" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article2" aria-hidden="true"></div></li></ul></div><div class="pagination u-position-relative u-padding-s-bottom"><span class="u-position-absolute"></span><span class="pagination-pages-label"><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class="u-position-absolute"><button class="button-link button-link-secondary next-button" type="button"><span class="button-link-text">Next</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel u-margin-s-bottom"><header id="citing-articles-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle button-link-primary" aria-expanded="false" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Citing articles (1)</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="u-display-none" aria-hidden="true" aria-describedby="citing-articles-header"><div id="citing-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://doi.org/10.3390/f9080490" target="_blank"><h3 class="article-title ellipsis text-s" id="citing-articles-article0-title" title="Determinants of above-ground biomass and its spatial variability in a temperate forest managed for timber production">Determinants of above-ground biomass and its spatial variability in a temperate forest managed for timber production</h3></a><div class="article-source ellipsis">2018, Forests</div></div><div class="buttons"><button class="button-link button-link-secondary side-panel-details-toggle move-right" aria-describedby="citing-articles-article0-title" aria-controls="citing-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article0" aria-hidden="true"></div></li></ul></div></div></section><section class="SidePanel u-margin-s-bottom hidden"><header id="metrics-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Article Metrics</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><a href="https://plu.mx/plum/a/?doi=10.1016/j.biosystemseng.2018.07.003" class="plumx-summary plum-sciencedirect-theme" data-pass-hidden-categories="true" data-hide-usage="true" data-orientation="vertical" data-hide-print="true" data-site="plum" data-on-success="onMetricsWidgetSuccess">View article metrics</a></div></section></aside></div></div><div></div></div><div id="footer"><div class="hor-line" style="border-color:#e9711c"></div><div class="panel-s u-padding-l-bottom u-bg-white u-clr-grey7" role="contentinfo"><a class="anchor move-left els-footer-elsevier anchor-has-inherit-color" href="https://www.elsevier.com/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text"><svg viewBox="-3345 3440.027 140.01 24.333" style="width:104px;height:30px"><title>Elsevier</title><path id="E" style="fill:#E9711C" d="M-3343.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3344L-3343.999,3461.698"></path><path style="fill:#E9711C" d="M-3325.448,3461.698c2.074-0.118,2.83-0.502,2.83-2.832v-13.511c0-2.33-0.757-2.715-2.83-2.832v-0.591h8.884 v0.591c-2.243,0-3.027,0.472-3.027,2.891v13.009c0,1.652,0.056,2.625,1.71,2.625h4.008c3,0,4.4-2,5.576-4.1l0.673,0.118 l-1.71,5.222h-16.114V3461.698"></path><path style="fill:#E9711C" d="M-3307.122,3456.27h0.561c1.12,2.596,2.886,5.4,5.94,5.4c2,0,3.672-1.3,3.672-3.334 c0-1.652-1.626-3.1-4.176-4.927c-3.28-2.36-5.41-3.746-5.41-6.43c0-3.422,2.55-5.517,5.633-5.517c2.214,0,3,0.944,4.204,0.944 c0.476,0,0.56-0.266,0.476-0.737h0.561l0.645,6.076h-0.561c-0.785-2.625-2.523-5.19-5.354-5.19c-1.737,0-3.138,1.356-3.138,3.185 c0,1.918,1.933,3.157,5.016,5.016c2.523,1.504,5,3.362,5,6.254c0,3.245-2.608,5.752-5.97,5.752c-2.02,0-4.148-0.855-4.68-0.855 c-0.336,0-0.7,0.207-0.756,0.649h-0.56l-1.094-6.282"></path><path style="fill:#E9711C" d="M-3293.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3294L-3293.999,3461.698"></path><path style="fill:#E9711C" d="M-3265.839,3462.524h-0.42l-5.41-12.538c-0.896-2.065-1.71-4.16-2.83-6.136 c-0.478-0.826-1.346-1.327-2.3-1.327v-0.591h8.323v0.591c-0.785,0-2.354,0-2.354,1.15c0,0.384,0.87,2.45,1.653,4.308l4.063,9.676 l4.877-11.359c0.588-1.356,0.757-2.094,0.757-2.713c0-0.618-0.673-0.974-2.13-1.062v-0.59h5.941v0.591 c-0.337,0.06-0.7,0.117-1.037,0.295c-1.066,0.56-2.13,3.57-2.635,4.749l-6.5,14.957"></path><path style="fill:#E9711C" d="M-3255.472,3461.698c2.24,0,3.025-0.473,3.025-2.892v-13.393c0-2.42-0.784-2.89-3.025-2.891v-0.59h9.078v0.591 c-2.24,0-3.025,0.472-3.025,2.891v13.393c0,2.42,0.784,2.892,3.025,2.892v0.59h-9.08v-0.59"></path><path id="E_2_" style="fill:#E9711C" d="M-3244.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3245L-3244.999,3461.698"></path><path style="fill:#E9711C" d="M-3206,3461.698c-1.26-0.354-1.71-0.68-2.466-1.623l-6.166-7.609c3.027-0.65,5.13-2.185,5.13-5.547 c0-4.75-4.26-4.986-7.764-4.986h-9.191v0.591c2.24,0,3.026,0.472,3.026,2.891v13.393c0,2.42-0.785,2.892-3.026,2.892v0.59h9.08 v-0.59c-2.242,0-3.027-0.473-3.027-2.892v-5.604h2.551l7.314,9.086h4.54L-3206,3461.698 M-3220.399,3444.499 c0-1.387,0.337-1.476,2.186-1.476c2.774,0,5.3,0.974,5.3,4.308c0,3.6-2.887,4.63-5.914,4.631h-1.569v-7.463H-3220.399z"></path></svg></span></a><div class="panel-s u-bg-white u-padding-0-hor-from-xs u-padding-s-hor-from-md u-padding-xs-ver text-xs u-clear-both-from-xs u-clear-none-from-md"><p class="u-margin-xs-bottom"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/solutions/sciencedirect" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">About ScienceDirect</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.sciencedirect.com/customer/authenticate/manra" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Remote access</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.sciencedirect.com/science?_ob=ShoppingCartURL&amp;_method=display&amp;md5=3ff44acb300f01481824c54a2973d019" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Shopping cart</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Contact and support</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Terms and conditions</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/privacy-policy" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Privacy policy</span></a></p><p id="els-footer-cookie-message">We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the<!-- --> <a class="anchor u-margin-0-right" href="https://www.sciencedirect.com/legal/use-of-cookies" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">use of cookies</span></a>.</p><p id="els-footer-copyright">Copyright © <!-- -->2019<!-- --> Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.</p></div><div><span style="margin-top:-10px" class="u-position-relative move-bottom u-float-left-from-xs u-float-right-from-md move-right u-padding-0-hor"><a class="anchor els-footer-relx anchor-has-inherit-color" href="https://www.relx.com/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text"><svg style="width:118px;height:28px" viewBox="-3248 3424.938 334.44 53.422"><title>RELX Group</title><path style="fill:#E9711C" d="M-3227.285,3448.329c10.948,0,24.15-2.576,24.15-13.124c0-7.474-7.63-10.267-14.6-10.267 c-10.647,0-30.265,5.926-30.265,26.96c0,10.555,8.034,16.643,20.778,16.643c13.474,0,23.266-8.768,26.26-20.535 c-7.912,13.648-17.972,17.896-26.18,17.896c-10.19,0-14.077-7.568-14.077-14.05c0-18.56,14.406-24.798,23.494-24.798 c6.135,0,9.727,3.936,9.727,8.253c0,11.918-17.17,11.578-25.008,11.578c-0.803,0-3.274,0.02-3.76-0.003l-1.654,1.714 c3.59,0.508,8.554,1.497,13.944,4.223c11.204,5.656,21.745,20.59,35.7,20.59c5.062,0,6.38-1.543,7.335-2.893 c-6.834,4.354-18.286-3.197-24.46-9.04c-5.082-4.806-9.57-10.26-21.387-13.146"></path><g><path style="fill:#666666" d="M-3158.73,3468.502c-0.423,0-0.636-0.164-0.794-0.547l-6.885-14.511h-6.197c-0.21,0-0.316,0.11-0.316,0.33 v14.182c0,0.328-0.213,0.547-0.532,0.547h-5.189c-0.32,0-0.53-0.22-0.53-0.547v-36.139c0-0.33,0.21-0.55,0.53-0.549h13.928 c6.62,0,11.492,4.545,11.492,11.117c0,4.873-2.7,8.65-6.833,10.238l7.575,15.167c0.21,0.385,0,0.713-0.37,0.713L-3158.73,3468.502 L-3158.73,3468.502z M-3159.42,3442.384c0-3.232-2.223-5.31-5.506-5.311h-7.68c-0.21,0-0.316,0.108-0.316,0.328v9.912 c0,0.217,0.104,0.328,0.316,0.328h7.68C-3161.644,3447.641-3159.42,3445.561-3159.42,3442.384"></path><path style="fill:#666666" d="M-3146.95,3431.817c0-0.33,0.21-0.55,0.53-0.549h21.865c0.317,0,0.53,0.22,0.53,0.549v4.709 c0,0.33-0.212,0.548-0.53,0.548h-15.827c-0.212,0-0.318,0.108-0.318,0.328v9.089c0,0.22,0.106,0.328,0.318,0.328h12.726 c0.317,0,0.53,0.22,0.53,0.547v4.709c0,0.33-0.213,0.548-0.53,0.548h-12.726c-0.212,0-0.318,0.11-0.318,0.329v9.418 c0,0.22,0.106,0.328,0.318,0.328h15.827c0.317,0,0.53,0.22,0.53,0.549v4.709c0,0.328-0.212,0.547-0.53,0.547h-21.865 c-0.32,0-0.53-0.22-0.53-0.547C-3146.95,3467.956-3146.95,3431.817-3146.95,3431.817z"></path><path style="fill:#666666" d="M-3118.2,3431.817c0-0.33,0.212-0.55,0.53-0.549h5.191c0.317,0,0.53,0.22,0.53,0.549v30.553 c0,0.22,0.106,0.328,0.317,0.328h14.722c0.318,0,0.53,0.22,0.53,0.549v4.709c0,0.328-0.213,0.547-0.53,0.547h-20.76 c-0.317,0-0.53-0.22-0.53-0.547C-3118.2,3467.956-3118.2,3431.817-3118.2,3431.817z"></path><path style="fill:#666666" d="M-3052.46,3449.885c0-6.46,0.423-8.926,1.006-10.787c1.747-5.53,5.878-8.377,11.758-8.377 c5.88,0,9.532,3.012,11.28,6.9c0.107,0.274,0.107,0.548-0.21,0.712l-1.695,0.876c-0.265,0.11-0.53,0.055-0.688-0.22 c-1.802-3.396-4.45-5.312-8.74-5.312c-4.45,0-7.468,2.192-8.792,6.3c-0.475,1.422-0.845,3.722-0.845,9.909s0.37,8.486,0.845,9.91 c1.324,4.107,4.343,6.3,8.792,6.3c4.342,0,7.468-2.137,8.79-6.19c0.477-1.422,0.85-3.56,0.85-7.117c0-0.22-0.106-0.328-0.318-0.328 h-8.155c-0.318,0-0.53-0.22-0.53-0.548v-1.807c0-0.33,0.212-0.548,0.53-0.548h10.962c0.317,0,0.53,0.22,0.53,0.548v2.243 c0,3.725-0.423,6.572-0.954,8.27c-1.694,5.53-5.88,8.432-11.65,8.432c-5.88,0-10.01-2.848-11.758-8.38 C-3052.036,3458.811-3052.459,3456.345-3052.46,3449.885"></path><path style="fill:#666666" d="M-3021.43,3468.502c-0.317,0-0.53-0.22-0.53-0.547v-25.023c0-0.33,0.213-0.548,0.53-0.548h1.801 c0.316,0,0.53,0.22,0.53,0.548v3.338h0.052c0.953-2.572,3.23-4.434,6.78-4.434c2.013,0,3.866,0.712,5.084,1.807 c0.265,0.164,0.317,0.438,0.107,0.713l-1.06,1.531c-0.212,0.274-0.48,0.274-0.795,0.11c-1.164-0.767-2.436-1.313-4.024-1.313 c-4.45,0-6.144,3.996-6.144,8.815v14.456c0,0.328-0.213,0.547-0.53,0.547H-3021.43L-3021.43,3468.502z"></path><path style="fill:#666666" d="M-3004.54,3462.26c-0.53-1.752-0.847-3.668-0.847-6.844c0-3.12,0.316-5.037,0.847-6.79 c1.378-4.327,4.818-6.79,9.426-6.79c4.662,0,8.104,2.464,9.48,6.79c0.53,1.752,0.848,3.668,0.848,6.79 c0,3.176-0.317,5.092-0.848,6.844c-1.377,4.326-4.818,6.79-9.48,6.79C-2999.722,3469.05-3003.162,3466.585-3004.54,3462.26 M-2988.387,3461.33c0.475-1.48,0.688-3.066,0.688-5.914c0-2.794-0.213-4.38-0.688-5.86c-1.007-3.12-3.442-4.873-6.728-4.873 c-3.23,0-5.666,1.752-6.672,4.873c-0.477,1.48-0.688,3.065-0.688,5.86c0,2.848,0.212,4.436,0.688,5.914 c1.006,3.12,3.442,4.873,6.672,4.873C-2991.829,3466.203-2989.395,3464.451-2988.387,3461.33"></path><path style="fill:#666666" d="M-2963.66,3468.502c-0.317,0-0.528-0.22-0.528-0.547v-3.066h-0.053c-1.272,2.574-3.92,4.162-7.36,4.162 c-5.615,0-8.74-3.616-8.74-9.913v-16.206c0-0.33,0.213-0.548,0.53-0.548h1.801c0.317,0,0.53,0.22,0.53,0.548v15.44 c0,5.257,2.118,7.83,6.515,7.83c3.812,0,6.78-2.74,6.78-7.284v-15.987c0-0.33,0.21-0.548,0.528-0.548h1.802 c0.317,0,0.53,0.22,0.53,0.548v25.023c0,0.328-0.213,0.547-0.53,0.547h-1.802L-2963.66,3468.502z"></path><path style="fill:#666666" d="M-2955.29,3478.36c-0.317,0-0.53-0.22-0.53-0.549v-34.879c0-0.33,0.213-0.548,0.53-0.548h1.8 c0.317,0,0.53,0.22,0.53,0.548v3.176h0.053c1.218-2.465,3.496-4.272,7.68-4.272c4.45,0,7.256,2.08,8.58,6.242 c0.638,2.137,0.9,4.435,0.9,7.393c0,2.9-0.264,5.2-0.9,7.336c-1.323,4.164-4.13,6.242-8.58,6.242c-4.183,0-6.46-1.807-7.68-4.27 h-0.053v13.031c0,0.33-0.213,0.55-0.53,0.549L-2955.29,3478.36L-2955.29,3478.36z M-2939.35,3461.602 c0.53-1.643,0.688-3.832,0.688-6.13c0-2.355-0.158-4.545-0.688-6.188c-0.952-2.957-3.124-4.6-6.46-4.6 c-3.178,0-5.507,1.533-6.46,4.6c-0.477,1.423-0.69,3.34-0.69,6.188c0,2.847,0.212,4.71,0.69,6.13 c0.954,3.067,3.283,4.602,6.46,4.602C-2942.473,3466.204-2940.3,3464.561-2939.35,3461.602"></path><path style="fill:#666666" d="M-3070.48,3468.545c-0.424,0-0.634-0.164-0.85-0.547l-7.254-12.649h-0.106l-7.307,12.649 c-0.213,0.383-0.424,0.547-0.85,0.547h-5.72c-0.37,0-0.528-0.328-0.318-0.711l10.76-18.562l-9.96-17.248 c-0.21-0.385-0.05-0.712,0.32-0.712h5.721c0.424,0,0.636,0.163,0.847,0.548l6.513,11.278h0.106l6.513-11.278 c0.213-0.385,0.425-0.548,0.85-0.548h5.72c0.37,0,0.53,0.327,0.317,0.712l-9.956,17.248l10.75,18.562 c0.21,0.383,0.052,0.71-0.32,0.711h-5.771H-3070.48z"></path><path style="fill:#666666" d="M-2932.08,3443.951c-0.108,0-0.18-0.075-0.18-0.189v-10.385c0-0.075-0.036-0.113-0.11-0.113h-3.202 c-0.11,0-0.184-0.073-0.184-0.186v-1.6c0-0.11,0.074-0.186,0.184-0.186h8.746c0.11,0,0.183,0.075,0.183,0.186v1.6 c0,0.113-0.07,0.186-0.183,0.186h-3.201c-0.073,0-0.11,0.038-0.11,0.113v10.385c0,0.113-0.07,0.19-0.18,0.189H-2932.08 L-2932.08,3443.951z"></path><path style="fill:#666666" d="M-2924.47,3431.478c0-0.112,0.07-0.186,0.178-0.186h1.604c0.163,0,0.252,0.056,0.307,0.186l3.367,7.8h0.07 l3.315-7.8c0.052-0.13,0.144-0.186,0.305-0.186h1.583c0.11,0,0.182,0.074,0.182,0.186v12.286c0,0.11-0.07,0.186-0.182,0.186h-1.564 c-0.11,0-0.18-0.075-0.18-0.186v-7.744h-0.073l-2.593,5.957c-0.07,0.168-0.18,0.242-0.343,0.242h-1.044 c-0.16,0-0.27-0.074-0.343-0.242l-2.59-5.957h-0.072v7.744c0,0.11-0.072,0.186-0.18,0.186h-1.567c-0.106,0-0.178-0.075-0.178-0.186 v-12.286H-2924.47z"></path></g></svg></span></a></span></div></div></div></section></div></div></div>
<script type="application/json" data-iso-key="_0">{"userAgent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0","abstracts":{"content":[{"#name":"abstract","$":{"xmlns:ce":true,"lang":"en","id":"abs0010","view":"all","class":"author"},"$$":[{"#name":"abstract-sec","$":{"id":"abssec0010","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"abspara0010","view":"all"},"_":"A stereovision system for the in-field estimation of trees parameters such as height and diameter is proposed. The system includes a specifically developed mobile application for the management and georeferencing of stereo images. Stereo imaging allows the measurement of the distance between two points using triangulation formulas for the extraction of three-dimensional coordinates. The methodology is structured following three phases: training using system calibration through stereovision analysis of known artificial known objects; testing using measurements of standing tree diameters and heights acquired through stereovision system, laser rangefinder (height) and tree calliper (diameter); field application testing using direct height and diameter measurements in natural and urban woods. For this last phase an Android application was developed. The results show that the error between direct measurements and those measured with both stereovision and traditional reference methods (laser rangefinder and tree calliper) were quite low: 6.8 ± 6.6% between direct and laser rangefinder height measurements; 5.8 ± 5.5% between direct and stereovision height measurements; 4.2 ± 3.0% between direct and stereovision diameter measurements. No significant difference was found between the different methods for estimating height and diameter. Around 200 images matched to stereovision acquisitions were acquired and georeferenced using the application."}]}]},{"#name":"abstract","$":{"xmlns:ce":true,"class":"author-highlights","lang":"en","id":"abs0015","view":"all"},"$$":[{"#name":"section-title","$":{"id":"sectitle0010"},"_":"Highlights"},{"#name":"abstract-sec","$":{"id":"abssec0015","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"abspara0015","view":"all"},"$$":[{"#name":"list","$":{"id":"ulist0010"},"$$":[{"#name":"list-item","$":{"id":"u0010"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0010","view":"all"},"_":"Stereovision for in-field estimation of tree height and diameter."}]},{"#name":"list-item","$":{"id":"u0015"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0015","view":"all"},"_":"Mobile Android app for the management and georeferencing of stereo images."}]},{"#name":"list-item","$":{"id":"u0020"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0020","view":"all"},"_":"Comparison of measurements - reference (felled trees) and stereovision."}]},{"#name":"list-item","$":{"id":"u0025"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0025","view":"all"},"_":"No significant differences between stereovision and traditional methods."}]}]}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"biographies":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"lang":"en","id":"kwrds0010","view":"all","class":"keyword"},"$$":[{"#name":"section-title","$":{"id":"sectitle0015"},"_":"Keywords"},{"#name":"keyword","$":{"id":"kwrd0010"},"$$":[{"#name":"text","_":"Stereo images"}]},{"#name":"keyword","$":{"id":"kwrd0015"},"$$":[{"#name":"text","_":"Image analysis"}]},{"#name":"keyword","$":{"id":"kwrd0020"},"$$":[{"#name":"text","_":"Tree diameters"}]},{"#name":"keyword","$":{"id":"kwrd0025"},"$$":[{"#name":"text","_":"Tree heights"}]},{"#name":"keyword","$":{"id":"kwrd0030"},"$$":[{"#name":"text","_":"Android application"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"experiments":{},"rawtext":"","authors":{"content":[{"#name":"author-group","$":{"xmlns:ce":true,"id":"augrp0010"},"$$":[{"#name":"author","$":{"id":"au1","orcid":"0000-0003-3711-1399","author-id":"S1537511018303180-65e5e77af9baf5e3b8ff1957a85eb985"},"$$":[{"#name":"given-name","_":"Corrado"},{"#name":"surname","_":"Costa"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"cor1","id":"crosref0015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"∗"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:corrado.costa@crea.gov.it","id":"eadd0010"},"_":"corrado.costa@crea.gov.it"}]},{"#name":"author","$":{"id":"au2","author-id":"S1537511018303180-87034b5734677470326afebcd2c1d640"},"$$":[{"#name":"given-name","_":"Simone"},{"#name":"surname","_":"Figorilli"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0020"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au3","orcid":"0000-0003-4630-8986","author-id":"S1537511018303180-d688bbc404216bbbb73b3b7966550a75"},"$$":[{"#name":"given-name","_":"Andrea Rosario"},{"#name":"surname","_":"Proto"},{"#name":"cross-ref","$":{"refid":"aff2","id":"crosref0025"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]}]},{"#name":"author","$":{"id":"au4","author-id":"S1537511018303180-59bed454e542117a7c548a670e65e086"},"$$":[{"#name":"given-name","_":"Giacomo"},{"#name":"surname","_":"Colle"},{"#name":"cross-ref","$":{"refid":"aff3","id":"crosref0030"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"author","$":{"id":"au5","author-id":"S1537511018303180-67865fb47fc03aee36a18654e74201c7"},"$$":[{"#name":"given-name","_":"Giulio"},{"#name":"surname","_":"Sperandio"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0035"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au6","author-id":"S1537511018303180-1a80c248195b4ab941c1182ddc7c18d4"},"$$":[{"#name":"given-name","_":"Pietro"},{"#name":"surname","_":"Gallo"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au7","author-id":"S1537511018303180-a5852e38aec33e707d0461f11bedac58"},"$$":[{"#name":"given-name","_":"Francesca"},{"#name":"surname","_":"Antonucci"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0045"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au8","orcid":"0000-0003-2035-1257","author-id":"S1537511018303180-e4c1d3f0cb1582a278d5d809b99be672"},"$$":[{"#name":"given-name","_":"Federico"},{"#name":"surname","_":"Pallottino"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0050"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au9","orcid":"0000-0001-8225-1724","author-id":"S1537511018303180-588d49ee38a91d75a85506a415af746a"},"$$":[{"#name":"given-name","_":"Paolo"},{"#name":"surname","_":"Menesatti"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0055"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S1537511018303180-73e88afba9ca80c7f748cffdcb083026"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Consiglio per La Ricerca in Agricoltura e L'analisi Dell'economia Agraria (CREA) - Centro di Ricerca Ingegneria e Trasformazioni Agroalimentari, Via Della Pascolare 16, 00015, Monterotondo (Rome), Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Consiglio per La Ricerca in Agricoltura e L'analisi Dell'economia Agraria (CREA) - Centro di Ricerca Ingegneria e Trasformazioni Agroalimentari"},{"#name":"address-line","_":"Via Della Pascolare 16"},{"#name":"city","_":"Monterotondo (Rome)"},{"#name":"postal-code","_":"00015"},{"#name":"country","_":"Italy"}]}]},{"#name":"affiliation","$":{"id":"aff2","affiliation-id":"S1537511018303180-981a30b20281692cec1990215303161e"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of AGRARIA, Mediterranean University of Reggio Calabria, Feo di Vito, 89122 Reggio Calabria, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of AGRARIA"},{"#name":"organization","_":"Mediterranean University of Reggio Calabria"},{"#name":"address-line","_":"Feo di Vito"},{"#name":"city","_":"Reggio Calabria"},{"#name":"postal-code","_":"89122"},{"#name":"country","_":"Italy"}]}]},{"#name":"affiliation","$":{"id":"aff3","affiliation-id":"S1537511018303180-3617df0062c78f6453d39b8803f80c29"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"Effetreseizero Srl, Spinoff CREA, Via Dei Solteri 37/1, 38121 Trento, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Effetreseizero Srl"},{"#name":"organization","_":"Spinoff CREA"},{"#name":"address-line","_":"Via Dei Solteri 37/1"},{"#name":"city","_":"Trento"},{"#name":"postal-code","_":"38121"},{"#name":"country","_":"Italy"}]}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"∗"},{"#name":"text","$$":[{"#name":"italic","_":"Corresponding author"},{"#name":"__text__","_":". Fax: +39 06 906 25591."}]}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff1":{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S1537511018303180-73e88afba9ca80c7f748cffdcb083026"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Consiglio per La Ricerca in Agricoltura e L'analisi Dell'economia Agraria (CREA) - Centro di Ricerca Ingegneria e Trasformazioni Agroalimentari, Via Della Pascolare 16, 00015, Monterotondo (Rome), Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Consiglio per La Ricerca in Agricoltura e L'analisi Dell'economia Agraria (CREA) - Centro di Ricerca Ingegneria e Trasformazioni Agroalimentari"},{"#name":"address-line","_":"Via Della Pascolare 16"},{"#name":"city","_":"Monterotondo (Rome)"},{"#name":"postal-code","_":"00015"},{"#name":"country","_":"Italy"}]}]},"aff2":{"#name":"affiliation","$":{"id":"aff2","affiliation-id":"S1537511018303180-981a30b20281692cec1990215303161e"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of AGRARIA, Mediterranean University of Reggio Calabria, Feo di Vito, 89122 Reggio Calabria, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of AGRARIA"},{"#name":"organization","_":"Mediterranean University of Reggio Calabria"},{"#name":"address-line","_":"Feo di Vito"},{"#name":"city","_":"Reggio Calabria"},{"#name":"postal-code","_":"89122"},{"#name":"country","_":"Italy"}]}]},"aff3":{"#name":"affiliation","$":{"id":"aff3","affiliation-id":"S1537511018303180-3617df0062c78f6453d39b8803f80c29"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"Effetreseizero Srl, Spinoff CREA, Via Dei Solteri 37/1, 38121 Trento, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Effetreseizero Srl"},{"#name":"organization","_":"Spinoff CREA"},{"#name":"address-line","_":"Via Dei Solteri 37/1"},{"#name":"city","_":"Trento"},{"#name":"postal-code","_":"38121"},{"#name":"country","_":"Italy"}]}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"∗"},{"#name":"text","$$":[{"#name":"italic","_":"Corresponding author"},{"#name":"__text__","_":". Fax: +39 06 906 25591."}]}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"body":{},"exam":{},"article":{"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Academic Press","id":"350"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":22,"openArchiveStatus":false,"openArchiveArticleCount":0,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S1537511018X00075-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1537511018X00075/cover/DOWNSAMPLED200/image/gif/edf4dc35ba8b23241d015e1e7984f7da/cov200h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1537511018X00075/cover/DOWNSAMPLED200/image/gif/edf4dc35ba8b23241d015e1e7984f7da/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"21784","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S1537511018X00075-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1537511018X00075/cover/DOWNSAMPLED/image/gif/9884283d06bcbd09b5bb57d8965b5605/cov150h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1537511018X00075/cover/DOWNSAMPLED/image/gif/9884283d06bcbd09b5bb57d8965b5605/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"14371","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S15375110.gif","sourceOpenAccess":false,"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S1537511018X00075-cov150h.gif"},"pii":"S1537511018303180","dates":{"Available online":"18 July 2018","Received":"28 March 2018","Revised":["20 June 2018"],"Accepted":"4 July 2018","Publication date":"1 October 2018"},"access":{"openArchive":false,"openAccess":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"document-references":30,"analyticsMetadata":{"accountId":"31739","accountName":"ASCR - CTR South Bohemian Biology","loginStatus":"anonymous","userId":"622212"},"cid":"272441","content-family":"serial","copyright-line":"© 2018 IAgrE. Published by Elsevier Ltd. All rights reserved.","cover-date-years":["2018"],"cover-date-start":"2018-10-01","cover-date-text":"October 2018","document-subtype":"fla","document-type":"article","entitledToken":"8B3D0A1E7E754042944F6B557B28A3E68221E83A1CF04488ADAE22EBF601A2C9399424D70B05C91E","eid":"1-s2.0-S1537511018303180","doi":"10.1016/j.biosystemseng.2018.07.003","first-fp":"126","hub-eid":"1-s2.0-S1537511018X00075","issuePii":"S1537511018X00075","item-weight":"FULL-TEXT","language":"en","last-lp":"133","last-author":{"#name":"last-author","$":{"xmlns:dm":true},"$$":[{"#name":"author","$":{"xmlns:ce":true,"id":"au9","orcid":"0000-0001-8225-1724","author-id":"S1537511018303180-588d49ee38a91d75a85506a415af746a"},"$$":[{"#name":"given-name","_":"Paolo"},{"#name":"surname","_":"Menesatti"},{"#name":"cross-ref","$":{"refid":"aff1","id":"crosref0055"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]}]},"normalized-first-auth-initial":"C","normalized-first-auth-surname":"COSTA","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-embargo-opening-date","_":"2020-07-18T00:00:00.000Z"}]},"pages":[{"last-page":"133","first-page":"126"}],"srctitle":"Biosystems Engineering","suppl":"C","timestamp":"2018-10-05T15:14:43.693083Z","title":{"content":[{"#name":"dochead","$":{"xmlns:ce":true,"id":"dhead0010"},"$$":[{"#name":"textfn","_":"Research Paper"}]},{"#name":"title","$":{"xmlns:ce":true,"id":"title0010"},"_":"Digital stereovision system for dendrometry, georeferencing and data management"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"174","vol-iss-suppl-text":"Volume 174","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":false,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":true,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"issn":"15375110","issn-primary-formatted":"1537-5110","useEnhancedReader":true,"isCorpReq":false,"pdfDownload":{"linkType":"DOWNLOAD","linkToPdf":"/science/article/pii/S1537511018303180/pdfft?md5=8899bcbf3b23c97f1028cdb556afda8c&pid=1-s2.0-S1537511018303180-main.pdf","isPdfFullText":false,"fileName":"1-s2.0-S1537511018303180-main.pdf"},"pdfUrlForCrawlers":"https://www.sciencedirect.com/science/article/pii/S1537511018303180/pdfft?md5=8899bcbf3b23c97f1028cdb556afda8c&pid=1-s2.0-S1537511018303180-main.pdf","indexTag":true,"volRange":"174","issRange":"","userProfile":{"departmentName":"Library","accessType":"IPRANGE","accountId":"31739","webUserId":"622212","accountName":"ASCR - CTR South Bohemian Biology","departmentId":"47969","userType":"NORMAL","hasMultipleOrganizations":false},"entitlementReason":"package","articleEntitlement":{"entitled":true,"usageInfo":"(622212,U|47969,D|31739,A|936,S|20,S|34,P|2,PL)(SDFE,CON|a30863ea9ce018477b4ac8d08be0e378262fgxrqb,SSO|ANON_IP,ACCESS_TYPE)"},"aipType":"none","downloadFullIssue":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/app/home/supporthub/sciencedirect/","contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","orgName":"","webUserId":"622212","libraryBanner":{},"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasMultiOrg":false,"userType":"IPRANGE","allowCart":false},"titleString":"Digital stereovision system for dendrometry, georeferencing and data management","onAbstractWhitelist":false,"isAbstract":false,"isContentVisible":false,"ajaxLinks":{"citingArticles":true,"references":true,"referredToBy":true,"toc":true,"body":true,"recommendations":true}},"specialIssueArticles":{},"recommendations":{},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"citingArticles":{},"workspace":{"isOpen":false},"crossMark":{"isOpen":false},"userIdentity":{},"refersTo":{},"referredToBy":{},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[]},"references":{},"referenceLinks":{"internal":{},"external":{}},"glossary":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenRecommendations":true,"isOpenCitingArticles":false,"citingArticles":[false,false,false],"recommendations":[false,false,false,false,false,false],"specialIssueArticles":[false,false,false]},"banner":{"expanded":false},"transientError":{"isOpen":false},"tableOfContents":{"showEntitledTocLinks":true},"chapters":{"toc":[],"isLoading":false},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"metrics":{"isOpen":true},"signOut":{"isOpen":false},"issueNavigation":{"previous":{},"next":{}},"tail":{},"linkingHubLinks":[],"signInFromEmail":{"isOpen":false},"clinicalKey":{},"accessOptions":{}}</script>
      
      <script src="satelliteLib-b7cfe8df39a4e5eec5536bba80e13f4b6fa0dd7c.js"></script><script src="satellite-565e008964746d4385002642.js"></script>
      <script src="babel-polyfill.js"></script>
      <script src="react.js"></script>
      <script src="react-dom.js"></script>
      <script src="arp.js" async=""></script>
      <!-- begin usabilla live embed code -->
      <script type="text/javascript">
        window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
        window.usabilla_live = lightningjs.require("usabilla_live", "https://w.usabilla.com/eb1c14a91932.js");
        var customData = {};

        if(window.pageData && pageData.content && pageData.content[0]) {
          customData.entitlementType = pageData.content[0].entitlementType;
        }
        if(window.pageData && pageData.visitor) {
          customData.accessType = pageData.visitor.accessType;
          customData.accountId = pageData.visitor.accountId;
          customData.loginStatus = pageData.visitor.loginStatus;
        }
        usabilla_live("data", {"custom": customData });
      </script>
      <!-- end usabilla live embed code -->
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="MathJax.js"></script>
      <script async="" src="gpt.js"></script>
    
  <div class="js-react-modal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><script src="widget-summary.js" async=""></script><div class="usabilla_live_button_container" tabindex="0" style="position:fixed;width:117px;height:34px;z-index:600;bottom:0px;right:2%" aria-label="Usabilla Feedback Button"><iframe src="" style="width: 117px; height: 34px; margin: 0px; padding: 0px; border: 0px none; overflow: hidden; z-index: 9998; position: absolute; left: 0px; top: 0px; box-shadow: 0px 0px 0px; background-color: transparent;" marginwidth="0" marginheight="0" scrolling="no" data-tags="bottom" title="Usabilla Feedback Button" frameborder="0"></iframe></div></body></html>